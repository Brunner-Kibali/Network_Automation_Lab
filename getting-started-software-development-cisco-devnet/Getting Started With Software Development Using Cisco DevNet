Course Overview
Course Overview
Hi, everyone. My name is Nick Russo and welcome to my course on getting started with software development using Cisco Definite. This course primarily follows a company's software development of a Web application and all the peripheral design and technical considerations. Along the way, you'll learn the fundamentals of software development, including development methodologies and design patterns, understanding the different types of AP eyes version control using local and remote git repositories. Basic http Operations and components. How to utilize the Cisco Developer Network Resource is and using python to interact with a really life a p I. After completing this course, you'll understand how to design, develop and manage simple software projects. This is a beginner's course, so there aren't many prerequisites. However, I'd recommend the following course. You'll need basic Python programming skills first. This includes data types, collections, loops, functions and file management. I hope you'll join me on this journey toe. Learn more about software development and AP eyes in the context of Cisco Definite at Pluralsight.

Learning the Foundations of Software Design
Introducing Globomantics and Other Things You Should Know
Hi, everyone. My name is Nick Russo and welcome to Pluralsight course on software development. In the context of Cisco's developer network, better known as definite, this first module introduces the foundation's off software development and design. Let's dive in. Here's the lineup for this module. I like to start my courses by setting expectations up front. There's nothing worse than getting halfway into a course and realizing it's not for you. We can't do anything fun with code until we know how to interact with the Bash Shell. I'll take you through a quick start demo here. A mistake many programmers make is _______ on the keyboard before they have a plan. I'll show you how to avoid that. And yes, this includes demos. As an important aside, I'll teach you how to set up your environment for python development. Using common tools, we'll wrap up with the discussion by reviewing a simple Web app using Pythons flask framework, which will follow us through the course. This is a beginner's course, so I'm not expecting you to be rock stars, but there are a few things you should know. First, you should understand the basics of python this includes data types, loops, functions, modules and the general syntax of the language. You don't need extensive experience as this isn't a python coding class, but we are going to write some code. I try to keep things simple wherever I can, sometimes all right, complex code by necessity and guide you through the high level operation of it while offering simplified explanations. I'm only asking that you be accustomed to following the codes flow without necessarily understanding every line. Python is, Ah, pretty English friendly language. So I don't suspect this will be a problem for anyone. I also insert extensive comments into my code to improve readability. Last, you should have some experience working in i t. It doesn't matter if you are a system administrator, network engineer, storage wizard que a tester or software developer just participating in an I T department and understanding the general rolls. Responsibilities and interactions between the various functional areas is fine. Even though this is a beginner's course, you can easily drill deeper into the topics that interest you. Maybe you find the python programming and software design aspects of the course to be the most interesting Pluralsight has dedicated courses. For both of these topics, just make a note of the topics you enjoyed the most as you progress through the course so you can revisit them by browsing the Pluralsight library. Now I'll introduce the scenario that weaves its way into this course. These are the regional sales managers at Global Mantex. You've recently been hired as the software team leader for a new customer relationship management or C R M Project Sierra. Um, applications are typically used by professional salespeople. Toe help transform prospects into paying customers. The company is forecasting big increases in sales over the next several years once the Sierra map is deployed. You've got a good team of developers working with you, but they'll need your guidance in leadership to build this app the right way. Because this app is being developed on Lenox, we'll explore some basic Lennox commands next

Demo: First Things First; Basic Bash Skills
The first thing you'll need to learn is how to interact with machines without a fancy graphical interface. So let's explore a popular shell program called Bash. Before we type any commands. Let's explore a bash shell prompt many systems used by default. It shows our current user name, which is easy to user, followed by the host name of the device to which we are connected. Then it shows the name of the directory we are currently in, which is global man ticks. It would be useful to learn more about where we currently are in the directory hierarchy. The command PWD or Print Working directory is a good starting point. In Lennox, forward slashes are used to identify directories with forward slash by itself representing the root directory. Now that we know where we are, let's see what files are in this directory. We can use l s short for list to reveal the contents. We see two files named file one that t X T and filed to that T X T. Perhaps we want to get more details about these files. Lennox has a built in manual page for almost every command, and we can use man. L s to check the docks for l s. From here, we can scroll up and down to read the details and use cue to quit. Specifically, we want to learn about the dash L option, which is the long format. This for both output will reveal additional information about each file. Thes codes on the left represent file types and permissions, which is a complex topic we won't be digging into today. The next two columns show the files owner and also the group to which the file is associate ID. If multiple users need to access the file, you could apply group permissions. Next. We have the file size in bites, and both of these are small files. Here is the last modified time effectively showing when the file's contents were last updated last we have the file names from earlier. Let's create a directory for these files. We used the M. K D I R Command to make a new directory. Okay, we created a new directory and notice it has a little D. As the first character in the detailed L s output To navigate into this directory, we can use the C D command to change directories. We can use PWD again to check our location and notice our shell prompt changed. Going forward is easy. But what about backward? If we use L s Dash A, which reveals all hidden files, we'll get a clue. In Lennox, everything looks and feels like a file even directories, hardware, components and running processes. The DOT directory represents the current directory, and this becomes handy one running shell scripts, a topic covered in a future clip. The dot dot directory represents the directory one level up. So if we see d into the dot dot directory, it should take us back one level. Okay, Now we're back where we started. Let's move our files into the new directory. We can use the M V Command to accomplish this, supplying the source file first and the target directory. Second, we can use at last tow list the contents of the directory from here. Even if we aren't in the directory, let's move file too in next, but also change its name in the process. The M V command is also used in Lennox to change file names. Okay, I changed back into the directory, used L s and everything looks good. We can also copy files using the CP Command. It works like M V except keeps the source file intact. I'll copy file one dot t x t to a file named coffee dot t x t. Now we have three files and notice that file one dot t x t and coffee dot t x t have the same file size. As expected. We can use the cat Command, which means can captain eight to print the file's contents. You can also use Tab completion to reduce typing. Let's check the other one to keeping in mind. We can specify multiple files and CAT will contaminate their outputs. The 1st 3 lines are from our coffee file, as we saw earlier, and now we have four new lines from our my Bash file. If you are brand new toe Lennix and want to edit files, I suggest the Nano editor. It's simple to use, and I'll demonstrate a quick change. You navigate with arrow keys and just make edits as needed. When done, Use control X then why? For yes to save the changes, we see the changes along with a new file size and modification time. I personally use of him throughout this course. Ah, more advanced text editor. And you can learn more about that using existing Pluralsight courses. Let's remove these silly files now, using the R M command. Okay, the file is gone now, and we confirmed with L s. We can use the dash. I option for an interactive removal using Why To confirm. Okay, no files left after deleting everything. Next, I'll move up a directory and delete the directory we were sitting in using the r m D I. R. Command. This is a safe way to remove directories. Next. Let's talk about environment variables. These air dynamic variables that can be specific to your shell process or globally accessible on the system. They're used to describe the environment it and modify the runtime behavior of applications. As I'll demonstrate in a future course, you can store secrets here for easy access to see the current environment variables. Use E N V. It could be a long list, and many of these are system defaults and things you likely won't mess with. Let's add a new one. Maybe we can store a secret password. So one of our acts can access it instead of prompting us for a password every time we run it, we use the export keyword, followed by a variable name, an equal sign and a value. It is conventional but not required to use capital letters for environment variables. We can use E N V to ensure it was added. We can access the value using the Echo Command, which just prints to the consul and D referencing are variable. Using a dollar sign, we can delete the variable using the unset command, followed by the variable name. If we try to access it, we get no output. For an example of a well known environment variable, let's examine the shell path. This is a colon delimited list of locations toe look for binaries such as L S, C D and M V. The shells searches each directory to find a given command, and we can export a new path variable if we want to modify where are executed, bals are stored last. You should understand package management When you want to install, update or remove software packages, you can use the built in OS package manager on red hat based distributions. This is yum and on debian based distributions. This is apt. You'll need to run most of these commands as route, which is the super user on Lenox Systems and can do anything pre fixing a command with the word pseudo. Also pronounced sou do allows a user to run commands as rude, provided they are authorized to do so. I'll install a package called Tree that will use later in the course using the Witch Command, we can see where the binary was installed, which verifies that are shell path can find it. That wraps up our intro to bash, and you'll see me introduce new commands and techniques through this course and several future ones. I'll be sure to explain what I'm doing as we move along, so don't worry. Next, let's talk about software development.

Software Development Strategies
[Autogenerated] As the leader of a software team, you'll need to understand the different kinds of development strategies and select a suitable one for global Mantex. Let's begin with the waterfall strategy. This is the classic software development method. It contains several discreet stages which occur in sequence. The exact stages may vary by project, but the general flow goes like this. Requirements are gathered from customers through dialogue and focus sessions. The application is designed based on these requirements. Once designed, the coders get toe work on their keyboards to implement the application. After completion, the testing phase begins to ensure the application satisfies all customer requirements. Only after all these steps are complete is the application deployed, often months or years later. This strategy, like all the other strategies, has its ups and downs, assuming requirements never change. This strategy works well because all the key information is known upfront and can be trusted as design inputs. Each phase of the operation is well defined, with clear separation of duties and responsibilities. It's also easy to manage using clear milestones for delivery, even by an inexperienced management team. However, the strategy cannot cope with changing requirements as there is no iterative or continuous feedback process once the stage has been completed, Moving backwards for rework or new features is difficult. Testing code, long after mistakes have been made, often results in a low quality product. Problems discovered so late in the process are expensive and difficult to fix. In the mid 19 nineties, programmers were looking for alternatives to the monolithic waterfall approach. Thus agile was born. There are many kinds of agile methodologies, but I'll illustrate the popular scrum method. Imagine many waterfall processes which contained requirements, collection, design, implementation, testing and delivery. These cycles occur rapidly and over short periods of time, typically 2 to 3 weeks. This implies that customer interaction and feedback is continuous and recurring, as is software testing and delivery. Customers will realize value sooner, and the product is incrementally improved. During these coding cycles, which are called sprints, sprints begin with a few hours of planning, where features are taken from the backlog and committed for implementation during a given sprint. There is no long term detailed planning as each sprint is self contained. An independent after code is delivered at the end of a sprint. The team holds a retrospective meeting where they discuss what went well, what went poorly and what can be improved for next time. Yeah, Joe comes with its own sets of advantages and disadvantages, agile flourishes in an environment where change is constant and customers want value sooner via frequent saw for delivery. Given the lack of departmental boundaries, agile teams tend to work better together and our goal oriented. The sprint planning meeting, for example, is only a few hours long. This means more time delivering and less time talking. However, without frequent customer feedback, agile strategies cannot work ajar relies on interactive cooperative customers that regularly provide feedback. The old standby of customers saying no comments for me simply won't do. The focus of agile is on writing functional code, not comprehensive documentation, which increases the dependence on individuals. Managing a waterfall operation is easy, given the timelines and boundaries, but agile teams require that team leaders remain deeply involved and focused. The concepts of lean were first applied to manufacturing operations through the 20th century, but have recently been translated into software development and I t operations. I'll describe the combine methodology here. Sometimes combine is considered a form of agile, but let's not split hairs. Conman is similar to scrum except without the arbitrary sprint interval. It seeks to deliver software continuously perhaps several times a day and uses working process or W I P as a way to drive results in manufacturing. A convent card grants permission to build one component. If you run out of conman cards, you need toe wait for replenishment before producing more components in software. The number of tasks in process is limited to a small number, which focuses the team's attention. If the CONVEN card for a specific feature is not marked as working process such as bug to you cannot work on it, period. In i t. We typically use a digital Web based conv onboard or a physical board with colored sticky notes. This is advantageous, since working process is a leading indicator of delivery lead time. More W I ___ in the system means more delay in delivery, which is undesirable. Lean software development also promotes integrated testing for every feature as it is written, and often even before it is written rather than a dedicated testing phase, tasks should Onley flow from left to right, just like new feature to just did. Conv on boards can have more than three columns, but this is the simplest variant. As a side note, I use conman for personal time management, and I used it to produce this very coarse lien is quite similar to agile, but with a few differences. It is with scrum. We pull in 2 to 3 weeks worth of work in a batch, but with Khan Bon work flows continuously with only a handful of concurrent tasks. The batch size is constrained by our work in process lim. The need for a task scheduling leader is obviated as the focus is on the work at hand, not the detailed sprint planning. Because W E. P is minimized. Delivery time is also minimized. This can lead to very frequent software deliveries. The lack of a central authority on many teams requires team members to police one another. Some teams do this well, others not so much. Team members must a laser focused on moving CONVEN cards from left to right. This is true for agile as well, but in lean, it is critical toe on Lee work on what is on the conv onboard. No pet projects or multitasking allowed In my professional experience, I tend to see between three and seven cards for teams of 10 to 15 people. The point of lean is to reduce lead time as a result of reducing W P, but without compromising on quality shipping. Junk code quickly just means you are doing the wrong thing faster. I'll cover testing strategies in a future course, but you absolutely cannot neglect this.

Understanding the Core Agile Tenets
[Autogenerated] a good way to summarize Agile is to review the core tenets. These abstract concepts led to the creation of specific methods like Scrum and Kon Bon. It's important to note that all of the items listed on this slide are valuable. Agile. Simply values the items on the left more which are prioritized first, agile values, individuals and interactions over processes and tools. Complex software projects require teams of people, and the quality of their communications is often directly proportional to the quality of the software delivered. That is to say, teams that don't work together write software components that don't work together. The best processes in tools in the world cannot overcome dysfunctional teams. Edge also values working software over comprehensive documentation. I've spent 10 years working in government and writing a 300 page document is often Mawr important than producing a useful product. I'll never understand that, as I believe that delivering outcomes to customers is more important than completing supplementary delivery Bols. Somewhat similar to the first tenet agile values, customer collaboration over contract negotiation. The focus here is on customers, not team members. As discussed in the previous clip, Agile is a fast iterative process that relies heavily on customer feedback at a few of my previous jobs, I often be moaned, are legalistic approach to customer interaction. We were more concerned about liability than about helping our customers win in the marketplace. Last and probably most importantly, agile values. Responding to change over following a plan. Software development is both a tactical and strategic process. But following a known bad strategy, especially when it becomes obvious that the strategy isn't working, is a recipe for disaster. As customer requirements and design constraints change, software developers should adapt and overcome new obstacles that arise. These tenets are defined in the agile manifesto, which I've linked below. I'd recommend reading it a few times as it's very short, and it's so important in understanding the agile mind set.

The Three Pillars of Good Coding
I won't lie. I stole two of these, and the third I really didn't invent either. Still, these are the absolute foundation of any strong coding project. The first topic is functional decomposition. You can think of this like divide and conquer. If you have a large, complex piece of software, it makes sense to break the project into functional areas. Perhaps one part of the program handles the database storage actions, while another handles the Web front end. Some of the programming constructs used to actually implement this, such as functions and classes, will appear throughout the demos. In this course, this approach has the added benefit of reducing repeated code, improving reusability and shrinking the code base. Next error checking. If you interactively prompt a user to enter a number and she enters her name instead. How does your program react? Ah, smart response might be a usage message, asking her to try again and giving her a reason why her input was invalid. Ah, poor response would be exposing the underlying error codes to the user and crashing the program. Have you ever been happy to see a blue screen of death? Was the message useful? Probably not. The point is to ensure that your code is functioning correctly as the code runs in real time, kind of like the monitoring system built into an automobile. Those 1st 2 are tips I learned from a grizzled coder at my first coating job years ago, and I harp on them in many of my Pluralsight courses. This last one is higher level and suggest that we follow design patterns. Deploying design patterns is a strategy for writing great code that others can easily understand, and that follows an intelligent workflow. Here's how I like to think of design patterns. I see them as generic and re usable solutions that solve recurring problems in software development. Design patterns are like blueprints that guide us towards cleaner solutions based on best practices. The idea of design patterns is to reduce the snowflake effect of having to design custom work flows for every project when it's often unnecessary. Later, in this module, we will explore too popular design patterns, the observer pattern and the model view controller or M V C. Pattern. Many design patterns are applicable in object oriented environments. What does that even mean? 00 P is a style of programming that encourages the use of purpose built objects to represent the components of your code. For example, in our soon to be written Sierra map, maybe we will create objects that represent customers. What attributes does a customer have? Maybe a name and an account balance. We can also define methods that detail how these data attributes are handled. These attributes and methods are defined in a class, and I'm using pseudo code to illustrate. Ah, classes, a blueprint and an object is an instance of a class. If we create a customer class with the attributes discussed earlier, we can. In Stan. She ate this class to create our specific customers rather than pass around independent variables. To represent this data, we can encapsulate them into a single object. Classes can also be arranged hierarchically, which allows for data inheritance down the chain. But this is an advanced topic, and we won't be digging into it.

The Power of Python pip and virtualenv
[Autogenerated] I want to make sure I am teaching the right habits to all the new python coders out there. Let's discuss to supplementary but very important python tools. First, there is Pip. Pip is a package manager similar to Yum and Apt, which we briefly discussed during our bash demo With Pip, we can install python packages from a website called Pipi i dot org's known as the Python Package Index. It's very easy to use, and for many of our future demos, we will need to install some custom packages. For example, what if Cisco developed a package that makes it easier to interact with network devices or management systems? That would be nice. Tohave right. Pip makes it easy to install, remove and manage these packages. Ah, virtual environment and python is complex behind the scenes but conceptually simple. Imagine you, as the software team lead, are contributing to many projects. One project may require the newest version of a package, while another project requires a slightly older version to maintain backwards compatibility with a legacy application. Using python virtual environments, you can create an environment per project and installed a proper package versions in each environment. This allows you to easily toggle between projects from the back shell

Demo: Setting Up a Workspace with pip and virtualenv
[Autogenerated] this demo Explorers getting set up with pip and virtual environments to simplify development Going forward. These are core skills every python programmer should know. Let's begin with virtual environments. Note that the Tilda is used as a shortcut for a user's home directory before starting identify which Python version you'd like to use as an example. I have three versions of python available on my death box, and I'll use the newest one to create a new virtual environment. Used the Dash M option to specify a specific module, then use V. E N V, followed by the environment name. I like to keep my virtual environments in a pre existing directory called Environments Within my Home Directory. I also like short and descriptive names. So let's use G Man 37 to signify global Mantex development. Using Python 3.7. Let's quickly peek inside. Inside the virtual environment, there are several directories, but I want to dig into the bin directory. I noticed that the virtual environment automatically comes with Pip, preventing us from an explicit installation. We also have this activate file, which is a fancy bash script we can run within our shell. This updates are shells, configuration and environment variables toe point into this specific virtual environment to find python packages rather than the system directories, we can enter the virtual environment using this source command and specifying this file notice our shell prompt chains, as it now displays the virtual environment name we also don't need to specify are Python version anymore. As simply Python will be referencing the proper python binary. Let's explore Pip. Now we can use it to look at the list of installed packages using PIP list. We don't have much installed yet, and our pip is out of date. Let's quickly follow the recommended action toe. Upgrade it. This output is characteristic for any pip installed as it shows you the package is installed and uninstalled. We're going to use a Web framework named Flask for our global Mantex CR M app. So let's install that using pip install flask scrolling up. We can see that this package has many dependencies such as click and ginger, too. Each is installed along with flask. At the end, Pit provides a summary of what was installed along with their versions. Let's use pip list again to double check. Okay, now we see several packages and it's enough to get us started. I'll be using PIP throughout this course and future courses to install important python packages as they become necessary. PIP has many other commands, and I'll cover just one more Pip show, followed by a package name can provide detailed information about a specific package. So let's look into flask. I think the most important bits here are at the bottom, where dependencies and reverse dependencies are clearly shown, making it easy to figure out what packages are needed. Last Let's exit our virtual environment using de activate. This is basically an undo button on the source. Activate command. We ran earlier and allows us to gracefully exit a virtual environment, allowing us to enter another notice. Our shell prompt is back to normal. Now that wraps up our chat on virtual environments and pip. Next. Let's dive into some design patterns

The Observer Design Pattern
[Autogenerated] With our development environment now functional, let's first explore the observer pattern. I'll keep this short so we can spend more time in the demo. There are two conceptual components in this pattern. Subjects and observers. The subject is the entity being observed, such as a stock ticker or a clock on the wall. Sometimes these are called observable objects. The observers are the entities that are watching the subject. Thus the subject will maintain a list of observers and often defines methods to register new observers and unregistered old observers. The subject also defines a method to notify all observers whenever an event occurs. The observers passively watch the subject and wait to be notified of some action. Just like with people, we all react differently to the same event, and different kinds of observers can implement their update methods differently. The subject just needs to raise the signal that an event has occurred. An individual observers will act accordingly as a side note. Sometimes subjects are called publishers and observers are called subscribers, which is more intuitive for some people. Stick around for a concrete example in the next clip

Demo: Homemade Observer Pattern Code Review
[Autogenerated] the global Mantex accounting department is having a meltdown. Their software is poorly designed and isn't able to reliably notify clients that their bills are due. Can you take a break from your serum work and suggest an alternative high level design? I've collapsed this demo into a single file just to keep things simple. I've defined two types of customers. Business and consumer grade. Let's explore the business customer class first. Remember, this is a blueprint for what all business customer objects will look like. Also, remember that I won't be explaining every bit of python, syntax or styling in this course. I just want you to see the observer pattern in action without a microscopic focus on the code. This double underscore knit function is called a constructor, which is used to build a new object. It's like a homebuilder taking the blueprint, buying the materials and constructing the house. That house will have attributes like an address Ah, color number of bedrooms, et cetera. In this case, ah, business customer has an account I D. And some quantity of money owed. The constructor requires these parameters at the time of creation and stores. This data remember observers are also called subscribers, so they need to define an update method when the subject or the publisher needs to notify the subscribers it calls their update method. This method is likely different between observers. In this case, if this customer carries a balance and owes money, the program should call the Finance Department. I'm using a print statement for simplicity, but perhaps a more advanced operation could be inserted here scrolling down. Our consumer grade customer has many common attributes. The constructor is identical, so let's check the update method. Most consumers don't have corporate accountants on standby, so instead, our program should send a reminder email that their balances do. This action is notably different than the business customer, and this abstraction is the power of the observer pattern. This will become apparent when we look at our accounting system. The accounting system is the subject. The observable component to which the observers are registered noticed the constructor doesn't take any additional parameters, and initialize is a variable called customers into an empty set. Basically, it's a collection of zero elements. When a customer registers, it is added to the set. Likewise, when it unregistered, it is removed from the set. The real power is with the notify method. Simply put, this method loops over all registered customers and invokes their update method. Remember, different customers will do different things upon notification. Sometimes it's a robo call to the corporate office and sometimes a polite reminder email. The accounting system doesn't know or care. Individual observers implement this behavior. However. They want to see the potential relevance for global man ticks. We can run a few basic test cases in our main function. Dysfunction runs when we execute our script. First, I create four customers with unique account ideas and some balances. Ah, positive balance means that they owe the company money. Next, we create the accounting system, object and register all four customers manually. After this, our customer set should contain four elements when we run the notify method, which would normally occur in response to some event. This will invoke the update method on each of our observer objects, keeping in mind that the exact implementation of each update method will differ between observer classes, I also want to quickly demonstrate unregistered ring. Maybe a customer cancelled their service with us after paying their balance when another notify. Action happens. Customer, too, isn't notified as it isn't observing this subject any longer. Let's run the code and see what happens. Notice that our subscribers are printed out of order. This is irrelevant but expected as sets are on ordered collections. I'm showing the customer balances in orange, and we can see that Accounts 203 100 don't owe any money while Accounts 104 100 do account. 100 is a corporate customer, and they got a dreaded robo call. While Account 400 is an individual consumer that receives an email after customer to on registers, the next notification event notifies the remaining observers and the same update. Methods are executed. Feel free to run this code on your own to explore the observer pattern in greater depth. Let's explore another popular design pattern next

The Model View Controller (MVC) Design Pattern
[Autogenerated] the model view controller or M V C pattern is commonly used in Web applications. The purpose of this design pattern is to separate functionality between components. In an application, the model provides an interface to the data used by the application. This may be a class with methods to assist with reading and writing data from a remote database at a high level, you can think of the model as the back end of an application. The model may also describe how the data is formed so that the other APP components don't need to know or care. They only need to invoke the proper methods on the model object. The view often serves as the front end of the application. The word view is described from the perspective of the end user, and it is how the user views the app. In modern times. It is frequently a Web based front end that provides input text boxes and clickable resource is for user's. The view is designed for user interaction without any data processing. The controller acts is the glue between these components. It interacts with the model by invoking methods to read and write data. The controller also interacts with the view by collecting data from its Web forms when the user hits the submit button. In between these interactions, the controller will apply the application logic, such as number crunching or other useful activities. This pattern scales nicely because one can often add additional components. Tow any one area, for example, if user traffic remains the same. But each transaction contains more data. Perhaps we need to scale our database by adding more database instances or model objects to the design. This is very apt dependent, but I want you to understand the concept. The separation of duties also allows individual components to be swapped out, for example, choosing one style of database over another for performance reasons. Sometimes M V C is called model template View, or MTV, especially in the context of Web designs. Some purists focus on minor semantic differences between M, V C and MTV, but that's irrelevant for global Mantex. For consistency. I'll stick with envy. See, in this course

Demo: Simple MVC-based Flask Web App
[Autogenerated] global. Mantex has asked youto prototype the new Sierra map so your team can get working on it. Let's use a popular Web framework called Flask for our prototype as a reminder. There is quite a lot of complex python in this demo, so focus on the high level flow by creating this app early in the course. We can use it as a learning aid later on, although we care little about the actual implementation. First, let's talk about the view. I used Initialize ER to create the basic HTML, CSS and Java script files needed to build a basic website. I am not a professional Web developer and will not be exploring CSS or JavaScript at all in this course, but we'll take a quick look at HTML since this represents our view. This is a quick and dirty project with only one goal to help you understand, M. V c. Here are all the files in the project. We installed this tree package using the young package manager earlier in our Bash demo, and it provides a hierarchical visualization of a directory. Everything in the static folder came from Initialize er, and we won't dig into it I copied the initial Isar Index file into a directory called templates because I needed to introduce some basic variables to our Sierra map home page. Let's explore that index file. This is HTML, and we see a references to the static CSS files for styling. Let's jump down to the action. First I set a header on the home page indicating this is a very early version of the global Mantex See Aram App. Also on the front page. Inside of a Web form, I print a text entry box asking for the account i D Customers can use this box to retrieve their account balances from the system. This is followed by a submit button. The controller will handle that activity. But regardless of what happens, the view is goingto have to provide some kind of feedback to the user. After submit is clicked, there are two outcomes. If the account number is valid and exists in the client database, then the system will print out the balance. The syntax used here is ginger to a text template ING language commonly used for Web pages and other text formats. I discuss ginger to extensively in my answerable and Python network automation courses. But here we won't dig in. The if statement is rather intuitive, given a non none account, i D. The balance is printed otherwise, and Air Message is displayed rather than crashing the program. Next, let's look at the model to keep things simple. I'm not using a real database like SQL as this would only confuse learners while adding no educational value. Instead, I'm just statically defining some database entries in the constructor of my database class. This represents the model of M V C and provides methods for accessing the data stored here. Notice that I'm creating three accounts here, each with varying balances. Account 100 is the only one that carries a positive balance and owes us money. The balance method takes in the account I D. As a parameter, using the python get function is a handy way to access dictionary values if you aren't certain they exist. If the specified key exists, the value is returned. If not, then none is returned. Therefore, this method returns either the integer balance for a valid accounts and none otherwise. Computing the balance is simple subtraction between what is due and what's been paid. Ah, more professional database class would have many more methods and much more complex logic. But I'm trying to illustrate the design pattern. Last, Let's check out the controller first. I import key functionality from the flask package that allows me to start the APP. Render HTML templates and handle http requests will cover http in greater depth in a future module. So don't worry about understanding all the request types just yet. First, I create a basic flask app following standard implementation guidance, followed by creating a database object. The logic that follows is the app specific knowledge contained inside the controller component of the M V C design pattern. The index function is what will get called whenever a user browses to the Sierra Map home page represented by the forward slash string. This at Symbol line before the function is called a decorator and advanced python topic that modifies the way a specific function behaves. The word route in this context relates to the web. You are l browsed by the user. We also specify what kinds of http requests can trigger this method, such as get in post in simple terms. The http get is used when someone visits the website and the post is used when they click the submit button. As such, we will want to take different actions based on whether I get or Post was executed. If it was a post, let's collect the information from the text box and store it as account. Underscore i d. This is the controller pulling data from the view and now that the controller has the account, i d. It can pass it to the model. This should yield the account balance as the model can tell us this information for a given account I d. I also turn on a bit of logging just to provide some troubleshooting assistance in case things go sideways. It just prints the account i d and balance to the shell and users never see it. If anything other than a post was received, which in our case on Lee includes a get, then we will use N A as the account balance. To render the HTML template to the users, we used the render template function along with the specific template name. Remember, we called it index that html and it contained some ginger to conditional logic We can also pass in any user data, such as the account balance, so the view can display this to the user. I noticed that the controller logic did not collect user input nor display user output. It did not read from the database, nor right to it. It's simply used methods that the model in view provided to interact with those components. Let's run it and explore the Web interface at this point are flask. App is running on Port 5000. If you don't know much about networking, I'll cover network fundamentals in a future course. But this just means we need to specify Port 5000 in our Web browser. I'm using my personal domain of N J R U s m c dot net to provide a host name for this app, making it publicly accessible. This looks pretty good. Let's enter a valid account I d and click submit. Okay. When we hit, submit that triggered an http post which instructed the controller to read the text input check the database then passed the balance back to the view to be printed for the user account. 100 does indeed have a balance of 40 Let's try another account. 200 has a negative balance because they overpaid us. So maybe this serves as a credit for future purchases. What about a bogus entry of Nick? 123 Okay, the view responds appropriately, providing no balance and revealing that our account I d. Was invalid. Let's quickly go back and check our logs. We see several get requests early on, followed by post requests where we query individual accounts. This includes the corresponding balances to which are sometimes invalid. We can stop the application using control. See, the main point of this demo was to show you how the components interact in future modules. We will beef up this app with better database handling, testing and more.

Module Review
[Autogenerated] Let's quickly review what we learned in this module. The first big task we tackled was learning the bash shelf. This is a core skill you'll want to keep sharp for your entire professional career. I hope I was clear in saying that software development requires an appropriate methodology, good usage of design patterns and other best practices and, of course, highly skilled coders. Just be sure you have all of these because without all of them, you'll get a sub optimal outcome. I finished up by reviewing a simplified application built on the M V C concepts. This new global Mantex Sierra map will get a lot of attention for the rest of this course as we enhance it with structure, data and version control. Let's explore some of those enhancements in the next module.

Working with Structured Data and Local Version Control
Introducing JSON, YAML, and XML
[Autogenerated] in this module, we will enhance our existing Sierra map using a pair of handy technologies, structure data and local version control. We have lots to cover in this module. Our serum database from the previous module used statically defined python data inside the source code. We'll explore three different sin taxes of structured data so we can decouple our data from our code, focusing on Jason Yeah, Mo and XML. Now that we have many different files in our project, both code and data, I'll introduce using the get version control system to track changes. This includes several demonstrations where I'll teach you the basic get operations most commonly used. I'll finish up by explaining how to compare files using the diff mechanism, which is also built in to get. Do you remember our Bash demo earlier in the course? Can you see any problems with the output we received from it? Many shell programs, whether it's bash or a network device command line, will give you plain text in response to your commands. This is great for humans to read, but difficulty for machines, complex parsing techniques are normally needed to extract and process individual pieces of data if we use some kind of structure data, we could easily load that into memory without parsing. Think about the syntax required to build a python dictionary. If we type that into a file, how hard would it be to transform it into a python data behind the scenes? Probably not that hard. And this is the power of structuring our data. I hinted at this last point on the agenda slide. But pulling that static python data out of our code and into a separate data file improves the maintain ability of our project. Overall, in a large project, you might use a real database. But using simple files will illustrate the point just fine. For our use case, let's jump into a few examples. Suppose we want to maintain a list of our customers, along with their attributes for loading into our C Aram app. This example is JavaScript, object notation or Jason Jason. Dictionaries are wrapped in curly braces, much like python dictionaries. In fact, Jason for Manning is almost identical to a python formatting, both in syntax and semantics. Like python lists are annotated using square brackets, and in this case we have a list of dictionaries. Each dictionary in this list has three keys. They represent a mix off string integer and Boolean attributes, describing each customer between elements in a list. Akama is used just like Python. Then we have another customer with the same dictionary, key names, but different values. This structure would be very easy to load into a python, don't you think? Next, Let's check out Yamma LL, which is Yemen? Ain't markup language. A goofy recursive acronym. The M O is a super set of Jason, meaning it can do everything Jason can do along with some new features. One basic feature is comments using the hash symbol similar to python yellow files. Conventionally begin with triple dashes. The scoping enamel is done using white space. Also like python, you can technically use square brackets for single line lists or curly braces for single line dictionaries. But that's rare. Here we have a list of dictionaries again, The dash in Yemen is a point of confusion. For many, it indicates the beginning of a list element. So in this case, we have a list of two dictionaries with three keys each, although I recommend always quoting your strings. Yeah, mo doesn't require it unless the literal colon is contained in the string or you want to do fancy escaping. Just be aware of this caveat. You can optionally endure Yellow files with three periods, and this is often a good idea. Yeh Mo doesn't have any closing DL emitters so accidentally truncating data is a real problem with Yemen. Last we finish up with XML or Extensible Markup language. This is the most tedious to read and write, but also the most powerful. Maximal documents must begin with an XML header indicating the version and encoding style. They also must be encapsulated in a single element, which means we cannot have a list as our top most element. I've named mine of route, but you can use any name inside this route element. I have Maura nested items. Each item is a customer list element which contains three sub elements. You can probably see the resemblance to Jason and Jamel. Note that elements must be explicitly closed using the element name preceded by a forward slash XML has a secret weapon. It can embed metadata in each element, which are called attributes. I've modified the XML slightly to illustrate this with green text in Yemen or Jason. We would normally specify these attributes as keys, but XML gives you more options. In my experience, this is harder to read and makes it harder to convert into Yemen or Jason. I suggest avoiding it, but you should be aware of the capability as it will become relevant in a future course on network program ability.

Demo: Working with JSON in Python
[Autogenerated] the developers on our team are seeking guidance on better ways to manage data within this year, eh? Map. Let's see if we can retool it using Jason. First, let's explore the Jason file that represents our database. I've created a directory called Data, which can contain are structured data files. The D. B dot jason file contains the same data we usedto have in our database dot p y source code. Let's explore this Jason file. Not surprisingly, it looks identical to python syntax just spread over many lines. We have a dictionary with three keys, each one representing an account. Each key has a value of another nest. A dictionary with two he's These represent the amount paid and the amount. Do nothing new here. Let's check out our model source code. Now I'll jump down to the constructor first instead of statically assigning a dictionary to the self dot data attributes. I am now using Python to read from the Jason File. Notice that the constructor requires a path parameter now, which is supplied by the controller. The with statement allows us to open the file and automatically close it when we're finished. The open file is called a handle in this case, and we could pass this handle into the jayson dot load function. The Jason Library comes standard with python know pip installations needed. And Jason doubt Load reads and parses Jason from a file. It will automatically convert it to a python data structure with no manual parsing needed. The rest of the database class is exactly the same. We only change the manner in which account data is loaded. There is one other minor change in our controller source code. Remember that our database constructor needs to be supplied with the file path. Here we pass in data slash d b dot Jason as a string. So the model knows where to find the file. Let's do a quick manual test to ensure things still work. I'll start up the AP test out a few accounts, then check the logs. Okay, account 100 is showing the correct balance of 40 and account 200 is showing negative 10. Also correct. Ah, bogus accounts still generates an error message. Let's check the logs quickly. Okay, This looks identical to what we saw in the previous module, except we improved our code substantially. This sets global Mantex up for success later as integrating a remote database is a bit easier. Now let's explore using Gammell in the next clip.

Demo: Working with YAML in Python
[Autogenerated] everyone loves Jason and is familiar with it. But the team also wants to explore and easier to edit. Alternative. This demo migrates from Jason to Yemen. Python has a number of different Yemma libraries, but these do not come natively with python. Ah, popular One is known as Pie Amel and is commonly used because it is simple and works well. Let's quickly install it. Great, it installed with no problems. Let's check out the Umoh syntax Next, which is called D b dot yemma. Within our data directory at a glance, you can probably see the resemblance to Jason. Yemma also supports comments, and I've included one here. We have three keys in this top most dictionary representing accounts, then Nesta dictionaries to carry the amount paid and amount due for each account. This is the exact same data in the Jason file from the previous clip just encoded as Yemen. Okay, now let's check the model. We will load data from this yellow file in the constructor. I've commented out the Jason option but retained it in the code for comparison. Yamma looks and feels very similar as we import the Yemma library, then perform a loading action toe. Open the file handle using safe load instead of regular load with yam. Oh prevents attackers from injecting arbitrary code into our program through a Yum o Phile. In a future course, I'll detail how injection attacks work by exploiting weaknesses in the Sierra map. But not now. At this point, the self dot data attributes will have all the correct account data. Remember, our controller needs to tell the database class which database file to use, and we should select Yemen. The Grip Command searches a file for a string match, and I'm looking for the word path. The output returns all the lines that contain the string path in the file start dot p y. We can see Jason is commented out and Yemen is enabled. Well, again, let's quickly test the app. I'll keep this short to avoid excessive repetition. Okay, account 100 works fine, and a bogus account still returns and error. The advantage of Yemen over Jason is that it is easier to read and write, but it can sometimes take longer to parse, especially for huge files. Let's finish up by trying to use XML

Demo: Working with XML in Python
[Autogenerated] some developers and another faction want to try a more robust language with support for nested attributes. This will support future growth. Would XML work here? As with Yem? Oh, there are a number of XML libraries, but I'm opting for a simple one called XML to dicked. Let's install it using Pip, it will load XML into a python dictionary automatically. You might be wondering how XML attributes are handled. In that case, let's check out the XML first Axum Ellis chattier than Jason and Jamel. But this brings additional capabilities. I added an attribute named Extra, with a value of fun into the account 100 element. The rest of the XML is rather intuitive and follows the same nest, a dictionary structure we've seen with Jason and Yemen. Let's check out the database source code like the rest. We need to start with the constructor. I've commented out both Jason and Yemen now and added the proper XML code. First, I import XML to dicked, then use the parts function. This is roughly equivalent to the load actions seen earlier, except it takes an XML string. So I used handled out read to snag the contents of the file. I then referenced the root key to remove the top most dictionary structure, revealing the three item dictionary with account numbers to show you how attributes are handled. I'm printing out the structure, which will appear in the terminal. Let's use grip again to ensure our controller has selected the right database file. Okay, that looks right. XML is in use while Jason and Yeah, Malark commented out. Let's start the APP noticed the order dictionary that is printed out. XML attributes are added as additional key value pairs except the key name is proceeded with an at symbol, making clear that it wasn't attributes rather than an element value. This is an intelligent and clean way to translate XML attributes into python dictionaries. Now let's test the app. Okay, account 200 shows the expected balance of negative 10. Nick 123 still doesn't work, which is correct, XML, maybe chattier. But attributes can be handy in certain cases, although for this simple app, not so much future use of the C. R. M app will use the Jason method by default. Now that we've explored all the options

What is Git?
[Autogenerated] I've briefly discussed get in my answerable and Python network automation courses, but let's dig a little deeper here. Version control gives us the ability to travel backwards in time. If we accidentally introduced bugs into our code, we can revert our changes by checking out an older working version of our source code and simply continue coating from there. Most version control systems allow for development branching. This technique allows developers toe work on new features, while the stable master branch remains available for consumption by customers or testers. Master is the known good branch, never polluted by a developers working process. I want to stress that most version control systems are simple in theory but often have a steep learning curve. I'll teach you the basic skills with git so that you can use it with confidence on your next project. Just know that get is a tricky topic toe learn when you're first starting, so don't get discouraged. Get is a distributed version control system that has a local and a remote component. Let's focus on the local part For now, there are three main stages that describe the state of a file. Ah, file can be in your working directory, the staging area or committed in the local repositories. When you make changes to a file, those changes occur in the working directory and get observes those changes. If you want to keep those changes, the first step is to add them to the staging area using get ad. If you didn't mean to stage your changes, you can unstaged them using the get reset head command. All we're doing is changing the location of the file in the get process, not editing its contents. I'll explain the usage of Head mawr in the demo. Assuming you want to keep the changes, you can commit them to the local repositories using, get, commit and supply a commit message describing the updates. Even after changes are committed, you can travel back in time to an old commit using get reset, followed by the Commit I D, which is a shallow one. Hash. If this is confusing, don't worry. I really don't like to talk about Get in power point. If you prefer, feel free to take a screenshot of this slide as a quick reference for the upcoming demo

Demo: Exploring Basic Git Operations
[Autogenerated] Right now, the team is doing nightly manual backups of all source code. Can you show them a better way to version control their work? I'm assuming you've already installed. Get on your system, which can be easily done using most OS package managers. I'm revealing my get version before we begin to see a command and option listing. You can use git, dash, dash help or simply get by itself. First, we need to initialize the repositories using Get in it dot This will create a hidden dot get directory, which has all the inner workings of git You'll rarely need to poke around in here, so we won't focus on that. We contract progress using get status. This provides a wealth of information, including the current branch, which files were modified and instructions on how to continue. Right now, all of our files are untracked and shown in red. We should use get add to stage them and rather than specify each file we can use, get ad dot to recursive Lee. Add all files in this directory. Now we see the files tracked as new additions and in green text. Let's commit thes to our local git repositories using get commit with a commit message we can use. Get help toe. Learn more about a specific command. This opens a man page. Forget, commit and we can see the dash. M option is used to specify a message. Let's run the command now. After the files are committed, get reports that there is nothing to dio. We've successfully version control our files. Now let's break something. Perhaps simulating an error made by a junior developer, I'll add some bogus syntax errors into our controller code. Let's run the app and watch it fail. This is a simple problem that any of us could easily fix. But imagine if it were complex and nuanced. Let's see what get has to say gets seized the modifications to start dot p y and gives us tips on how to discard the changes using Get check out. Let's try get check out with the dash dash option. Okay, those horrible changes are gone, and we didn't have to search through a file manually to remove them. What happens if we accidentally stage those bogus changes? Let me add them back in real quick. Okay, so I broke the code again, but the changes are staged. Get check out won't work now, since this action on Lee applies to changes in the working directory. However, get status gives us a hint and suggests we use get reset head followed by the file name. What's that mean? Head is just a pointer and get And in our case, it points to our most recent commit on the master branch, which has the initial commit we did a few minutes ago by resetting to head. It allows us to retain our changes, but unstaged is the file. Let's try it. The reset output shows us all the files that were unstaged and the capital M means modified get status. Now reports These changes are in our working directory, which we can easily discard with Get check out again. Let's make an even bigger mistake by staging and committing these bogus changes we can use. Get reset to go back in time to an old commit to see our commit history. Let's use get log logs air made unique using a shallow one commit i d. A long string of hex character's. The log also details who made the commit and when, along with the commit message to reset to a certain point in time use. Get reset, followed by the first few characters of the committee i d. I tend to use the 1st 7 which has a good chance of being unique even in large projects, much like get reset Head are changes are unstaged and back into our working directory weaken. Discard our changes Now, once and for all that commit has been a race from our log as well. We went back to an old commit, effectively moving the position of head. Now we only have one commit in our history that wraps up our intro. To get next, we'll explore using multiple branches.

Demo: Concurrent Development with Branches
[Autogenerated] The development team wants to improve how balances are displayed but doesn't want to risk breaking the code base. Let's help them create a new branch for these changes. We want to add some new functionality to our APP. Global Mantex is an American company, so let's use the US dollar symbol when showing balances and show two decimal points to account for fractions of a dollar, we can build a new branch called dollars using the get check out command. I know you're thinking Wait, I thought check out was for discarding changes. Get check out is technically used to switch between branches, in other words, to check them out. When you check out your current branch with uncommitted changes, those changes are wiped away. In this case, we are creating a new branch and switching into it using the Dash B option Visualize Ing branches is tricky, but here is a handy command. This will print out to get log in a concise format and also shows our branches commit I DS commit messages and head location. This is a lot to type, so let's use a get alias. The's aliases are shortcuts for existing get commands. I'm going to add an alias named Hissed, using the git config command. Now we simply use get hissed to replace this long command branch. Names are shown in Green Master is our default branch and in many projects represents the stable baseline. Sometimes it is called the trunk or mainline branch to right now Head master and dollars are all pointing to the same commit I d. Using get branch. We can see that we are currently on the dollars branch. Any future commits will advance the dollars and head pointers together. I'm going to quickly implement the new feature in the database dot p y file. First, I compute the balance by converting the values into floats, not integers and performing subtraction. Then I use some fancy python string formatting to print out a dollar sign followed by a space followed by the balance with exactly two decimal points. If we run, get status. We should see modifications to this file. Notice that now we are on the dollars branch, not the master branch. So if we ruin the project weaken, jump back on the master branch with get check out Master, I'll add and commit these new changes using a single command known as the Express commit. Okay, it's been committed and there are no other pending updates. Let's check the log now. Our current location, represented by head, has been advanced and is on par with the dollars. Branch dollars is now ahead of master. In a sense, as it has the most recent updates and newest features, we can use the Tail Bash Command to quickly print the last 10 lines of a file. So let's confirm that our database dot p y file has these changes. Before we move further, let's quickly test these changes. I'll start the APP, then open a browser. Let's try account 100. Now it says $40.0 instead of just 40. That's reflected in the logs as well. If I switch to the master branch and use the tail command on the database file, we will see the older indigent based version See the difference. Check out the log now. Dollars is still ahead of master by one commit ________ moved backwards. We are on the latest commit of the master branch. Now suppose we want to merge these dollar changes into the master branch. We are already on Master, the correct based branch, and now we want to merge dollars into master. It's a simple, as get merged dollars Master was fast forwarded to catch up with dollars. We'll discuss other types of merges in future modules. We don't need the temporary dollars branch anymore. Since we've merged the changes into master, let's delete it now on Lee, the Master branch remains next. Let's explore a handy tool for comparing files.

Demo: Interpreting a Unified Diff
[Autogenerated] a junior developer, made changes to a file but can't remember what changes he made. Is there a way to tell? Let's explore the diff command, which is both a bash command, and I get command. I'll start with the get based one as it's a bit easier to understand. I've pre staged some changes to our database dot p y file we can use Get def to quickly see them. This format is known as a unified def. A common method to compare text files in the context of get the logic is quite intuitive as the red minus signs signify deleted text and the green plus signs signify added text. Since the last commit, for example, I regressed this code to stop using the get function and implemented the classic dictionary access mechanism. Instead. This counts both as a deletion and an insertion. I also deleted, returned none at the end of the function, let's explore the diff header next, the triple minus signs and triple plus signs at the top are specifying the from file and to file respectively in Get This is a bit confusing, but it's basically measuring changes from the previous file state to the current one. This will make more sense when we compare to separate files using the Bash Def Command. Next we have what is called Ah, Hunk. This is a block of text that is common in both files that zooms in on changes. A diff may have many hunks. If there are many changes scattered through the files, the header of the hunk can be tricky to read. The first set of values, preceded by a minus sign, represents the line numbers displayed in the from file. Since the text shown is generally common to both files, this header tells us which line numbers are being shown in the From file. We are showing nine lines, starting from Line 42 inclusive in the to file, preceded by the plus sign, we are showing eight lines, starting from Line 42 again inclusive. This gives you a frame of reference when comparing files. Let's discard these changes and explore the bash version. Now get different deals. No output. As there are no differences between our last commit and are working directory. I'll copy database dot P Y two database to dot p y so we can compare the files outside of get let's make some silly changes to database to dot p y. Let's see if the diff tool picks up those changes. This time we have two hunks. The first hunk captures the 1st 7 lines in each file. As I changed the name of the author, we see the minus and plus science to indicate the change in the text output. The second hunk starts at Line 47 shows the addition of an ELT statement to handle the return. None line at the top. Now the triple minus and triple plus headers make a bit more sense as it identifies both files explicitly. Let's run the same command except reverse the positional arguments passing in database to dot p y. First, the output is almost identical, except the pluses and minuses are swamped. The minus sign signifies changes that are present in the from file but not in the to file. The plus sign signifies changes that are present in the to file but not in the from file. Commit these two rules to memory, and you'll quickly become a diff master. Let's delete this file and wrap up. Okay, We've got a clean working directory. But what if we want to share our work with other colleagues? We'll cover that integration in the next module.

Module Review
[Autogenerated] we covered many technical topics in this module. Let's take a step back and review. We discussed three common types of data structuring in this module. Jason, Gammel and XML. In my opinion, XML is the most robust and capable. Jason is the most python IQ and simple to learn, and you Hamel is the easiest to read and write. It isn't uncommon for a project to use multiple formats, depending on the use case, so be sure to understand them all I introduced get at a basic level to provide source control for code projects. It works nicely on any text based files, not just code. This included the usage of the unified def, both in the context of git and for any pair of files. These are big time savers. Once you master their use, both structure data formats and get operations are topics that require constant exposure to build skills. If you make using them part of your daily routine, you'll quickly gain expertise in the next module. We'll explore. Get in more detail by integrating with remote repositories like get Hub

Creating and Managing Basic Github Repositories
Integrating Local and Remote Git Repositories
[Autogenerated] this module focuses on integrating with Get Hub. This is such a critical topic for any developer to know. So let's dive in. This module is very heavy on demos. I'll introduce the concept of a get remote repositories and discuss the basic commands used to interact with. It will also be creating a repo in get up for testing. The concepts of branching emerging also apply to remote repositories, and I'll explain how those work embedded into this process is a code review flow that I'll briefly explain as well. This is great for double checking work on shared projects. What happens when multiple commits seem to contradict one another? We will need to manually resolve these merge conflicts. Let's quickly recap the local get operations. Get contains a working directory, staging area and local repositories. We discussed several of these get commands in the previous module, and you should refresh them if you need more practice before continuing. We can extend this workflow by adding 1/4 logical entity to represent the remote repositories. The remote repo will be accessible across the corporate network or possibly the Internet. If you are using a public service such as get up code that is committed to the local repositories can be pushed up to the remote repo using Get push. You can think of this like an upload action in the reverse direction. There are two general choices. Get clone is used when you want to initially download a remote repo. Get pole is when you are actively involved in a project and want to download any updates committed by other developers and merge them into your working directory. This is a minor point, but I want to share a secret about Get Pole. As I explained in the previous slide, Get pull, fetches down changes from the remote repo and merges them into your local working director. Three. Well, there is a command called Get Fetch that polls changes down to your local repo but doesn't merge them into your working directory. We could perform a fetch first, followed by a get merge to move those changes into our working directory. This is what get pulled does behind the scenes, and I thought you should know it. Many developers prefer to use get pull for simplicity, and that's what we'll do today.

Demo: Adding Existing Code to a New Remote Repository
[Autogenerated] The global Mantex Step Team has been using local git repositories toe work on their individual parts of the project but need a way to centrally version control it. Let's build them a get hub depository. Well, reuse the module three code as it makes sense to continue where we left off. Remember, we updated the U. S dollar formatting using local branches. I'm already showing our commit history to set up a get of repo. I'll log into my get up Paige. This clip doesn't detail the administrative setup of Get Hub Accounts. Sssh keys, et cetera. I want to focus on the Sierra map instead. From here, I'll click on repositories to add a new one. Then we click the green new button. Let's fill out the name and description. First, we can choose whether we want this to be public or private. Normally, you wouldn't select public for company internal software, but I'll do it here because it's free and easy. We can optionally ad or read me, which I'll skip for now. At the top, you can choose between using https or S S H for secure transport between your death box and get hub https works without additional setup, but requires entering your user name and password with each remote interaction. I'll be using SS H instead. Let's explore the quick start options. If the repositories is brand new and you haven't written a line of code yet, you can copy paste the first text block and run it from your death box. The first line creates are Read me file by writing the name of the repositories into a file called Read me dot md Using shell redirection. This bash technique redirects output from the Echo Command into the contents of the read me file. Next we initialize. I get repo stage the read me and commit the read me. Then we add a get remote. This is a reference to a remote repositories called origin by convention Origin represents the remote depository from the perspective of the death box, meaning we can push up and pull down from origin. Here, the action is push as we are uploading the new read me. The second option is the one I use most. If you have existing code, as we do, all we need to do is add the remote and push up our changes I'm going to copy paste these commands just to prove that they work perfectly. This generates a lot of output, but the most important part is the last two lines. We created a new remote branch named Master and our local master branch will track it. I'll illustrate why this is useful. Later, check out the history now. Now we have this red branch named Origin Master. This is a remote branch, while the regular Green master is our local master. Remember that get is a distributed version control system and there could be hundreds of developers using this same remote depository. Let's jump back to get hub and ensure our code was posted. I'll click on the repo name and go back to the Repo home page. And there you have it. Here's our code. Get up is suggesting we add or read me. So let's do that. This is written in a language called Mark Down, which is an easy way of writing HTML styled text without the complex HTML syntax. I'll add a header in some text to keep it simple, then quit. Okay, that looks good. Let's add and commit this file as we normally would I'm not using branches here because I want to focus on get hub integration. See how our local master has advanced beyond the remote master. If other developers look at our get home page, they won't see this. Read me yet as we haven't pushed it up. Even get status tells us that our local branch is ahead by one commit. So let's do a push and sink them back up. Because I have my local master tracking the remote Master Aiken simply say, get push without specifying the branch names. Let's go back to get Hub and refresh. Get her will automatically render our read me file on the repo home page, making it look pretty to demonstrate a pole action. I'm going to show you something you should never, ever do. Weaken directly, edit files and get hub using the Web you eye, which is a remarkably dangerous thing to do. I'll click this pencil icon to edit the read me and add some additional text. We can preview the changes to see how it will look when rendered. I'm happy with that. Now we need to add a message and commit these changes. I'll commit directly on the master branch to keep it simple. Okay, changes committed. Let's go back to the main repo page. The changes are reflected on the front page. What about our death box? Our local repo has no idea that the remote has changed. So let's use get pull to snag these updates. As I said in the slides, A get pole is a get fetch followed by a get merge. First we fetch down the changes, then merge them from Origin Master into our local master. We fast forwarded our local master up toe where the remote Master Waas. We can clearly see the updated text in the Read me as well. In the next clip, let's integrate remote branches into the get hub process.

Demo: Adding new Features via Pull Request
[Autogenerated] As the global Mantex development team is finishing features on their individual feature branches, they need to merge them into master. Is there a safe and controlled way to do this? Suppose you want to push up a work in process feature branch so others can work on it too. Let's improve our documentation using this method. First, we need to check out a new local branch using get check out Dash B. I created the Read Me branch here and now we can update our read me. Let's add a note using the markdown greater than operator. Now I'll save these changes. Then use the express commit command toe update our local read me branch. As expected, our local read Me branch is ahead of both our local and remote master branches. We could merge our local read me into our local master here. But what if our other colleagues need toe work on the read me branch? Also, let's push it up using get push except specifying a new remote branch called Read me Notice the output shows a new branch being created along with remote branch tracking. This is the same output we saw when we did our initial push on master in the previous clip, Let's check out Get Hub Next, at the top of the repo home page, we see a note that we recently pushed to a branch called Read Me. If we want to explore the changes, we can select the branch from this drop down on this branch, we can see our updates to the Read me What if we want to merge these into master? We can open a new poll request. We can click the compare and pull request on the home page, or we could manually build one using the pull request tab. Let's click the home page button. Ah, pull Request or PR is a way to request Is that a remote branch be merged into another branch? In this case, we want to merge. Read me into master. Get up tells us that these branches can be automatically merged, meaning there are no conflicts on the right. We can assign reviewers and other metadata to enrich the PR. I'll keep it simple and assign myself. Then add some basic notes around why this PR is needed in a professional environment. You'd provide a clear explanation as to the necessity of the PR, such as fixing bugs or adding features before creating the pull request, Weaken, scroll down and compare the changes. In this case on Lee, a few lines were added within a single file. This is like a gooey based unified if this all looks correct, so let's create the PR. At this point, we can either accept or reject the PR in a multi person project. We might require a minimum of two reviewers before approving it as part of a code review process. Let's quickly review our own code. I can click on the commits tab, then examine what changes were made. Okay, we have one commit. So let's drill into it. As we saw when we opened the PR, here is the diff again. We can comment on individual lines or attach general notes to the PR. I'll add a line comment here. If we click review changes at the top, we can add a general comment, approved the PR or request additional changes. I'll add a comment here, too. Everything I just did is summarized on this PR page so other reviewers can easily see it. At this point. Let's merge this pull request. Just like with local branches, it's usually a good idea to delete unused branches after emerge as this is a safe operation. So now what? Back to the shell. Let's pull these changes down from Get Hub. Before I do that, let's switch to the master branch. Since we know the remote read me Branch was deleted. Now I'll use get pulled dash p to prune any deleted branches. When we perform our poll, check out the history. Now we are on the local master branch, which is sync with the remote master and is the newest code. We emerged from the Read Me branch and you can see ah hump in the branch history. We also have a merge commit here as PR based merges aren't fast forwarded. There are ways to disable that, but those are advanced topics. Let's delete that leftover local read me branch. Okay, this is nice and clean. Using P ours for code reviews and merging branches is a very common and powerful workflow. What happens when emerge goes sideways and get sees a conflict? Let's discuss that in the next clip

Demo: Handling Merge Conflicts
[Autogenerated] it was bound to happen. At some point, two different engineers changed the same lines of code in the same file and get can't reconcile these contradictory updates as the team leader is your job to sort this out, I'll paint a simple example here. We'll make some changes locally and commit them to our local repo. Then I'll commit the unforgivable sin off, modifying the same line of code directly from get hub and committing it. This second action will simulate another developer making a change and pushing it up to get hub. Suppose we received some complaints from our Australian customers that they cannot tell whether the dollar sign is US dollars or Australian dollars. I commented out the old format for comparison, but now we are a pending USD instead of pre pending a dollar sign. Let's use get def to see the changes. Okay, so we added the commented line, removed the old format and added the new format. Let's do an express commit Then check our get history. Okay? Our local master is now one commit ahead of the remote master branch represented as Origin Master. Notice that I haven't pushed this update yet. Perhaps we are still working on the feature and are taking a quick coffee break. In the meantime, let's jump over to get hub and simulate one of our peer developers, pushing up a conflicting change. I'm at the repo home page again, so I'll click on our database dot p y file and make a quick edit. This developer feels like we should just pre penned the word us to the dollar sign. After all, a capital A followed by a dollar sign means Australian dollars. So why not use a similar approach? Let's commit these changes. Okay, all done. Now let's get back Acto r Shell. At this point, what will happen if we perform? I get push or a get pole? I'll use get pulled to demonstrate, but the effect is largely the same. The operation quickly fails and get tells us that database dot p y has emerged conflict and tells us to resolve it. As always, get status gives us a clue. It reveals that both our local and remote master branches have changed this file. The branches have diverged, which isn't normally a problem, but because we modified the same lines of code get doesn't know how to continue. Maybe get def. Gives us another clue. We see a bunch of weird syntax added into our file. Remember, the green plus sign indicates unstaged text that is newer than head. Get added this text to our file and it's actually useful tohave. We can see arrows pointing left and right with different text associated with them. The left Aargh! Says Head, which indicates our latest commit on our local master branch. Beneath that, we see are U S D format, which is what we committed locally. The equal sign text is just a delimit er to separate the two options. The text beneath the equal signs is associated with a shallow one. Commit i. D. This is the commit i d. From the other developers commit where a different format was chosen. We can confirm this with get hissed. Looking at the 1st 7 characters from our history, we can't see the commit ID's do indeed, match also noticed that at the top of the get hissed output, we see a clear divergence between our local master and the remote master. We need to resolve the conflict by selecting one of these options and that requires manual editing. Let's assume that we called our colleague and explained why Rusdi option was the right choice. She agreed and asked us to resolve the conflict. All we need to do is delete everything else, which includes the get inserted syntax and the incorrect python code. Now we are left with the desirable outcome. As a quick check, we can specifically look at the difference between our local master and remote master Using this command, I am not checking for any uncommitted local changes, but rather comparing the latest commits between local and remote masters. From this output, we already selected the green text and now we just need to add and commit. Now we see an additional merge commit which reconciles the differences. Notice that the remote master branch is still behind to bring it up to speed. We can safely push up these changes. Finally, we can check our history and ensure that our local master and remote master are back in sync

Wrapping Up with Git
[Autogenerated] Let's quickly wrap up our discussion of git and get hub. When we say get is a distributed version control system, we mean that it has local and remote components that don't need to be in sync 100% of the time. This makes it more complex but also more flexible and powerful. If you want to introduce new features or fix bugs on a team project, use AH branch and pull request workflow. Other solutions like Get Lab called this a merger quest, but the concept is identical. The contributor is requesting that the project owner pulled the changes in once reviewed and approved As powerful as get can be. It cannot read your mind, so you'll want to be familiar with resolving merge conflicts. There are fancy tools to do this graphically, but I demonstrated the most ubiquitous method. Using a text editor coming up, we'll explore a P I's in the next module

Introducing Application Programming Interfaces (API)
Comparing Different API Styles
even if you are relatively new toe I t. You've probably heard the word a P I. I tossed around. Let's explore what that really means. AP eyes are a broad and deep topic and in this course will stick to the basics. I begin by describing the different types of AP eyes and explaining their trade offs. Pluralsight has many excellent courses on http. But I'll provide some practical tips to enable us to consume rest AP eyes in order to use AP eyes. In an interesting way, I take a detour and introduce a few Cisco specific details. First, I provide a quick virtual tour of Cisco's definite site and explain the resource is available there. Then I'll introduce one of Cisco's flagship products, known as the Digital Network Architecture, or D N a Center. Now that we understand a bit about Cisco Definite and DNA center, we can run a P. I calls against it last. I'll teach you how to interpret simple AP. I sequenced diagrams using the unified modeling language, or UML. I'll quickly define application programming interface or a P I with a simplified definition to fit our use case. When you use an A P I. To interact with some device. You have access to a set of operations built for standardized management of the system. These operations are easily consumed by programmers and utilize structure data such as Jason and XML. Although there are many ways to design in a P I rested P eyes are quite popular these days and for good reason. Rest or representational. State transfer is an architectural style, not a framework or protocol or a language. Rest presents resource is in a logical way, making them easy to access. For clients. Rest AP eyes are stateless. This means that every request sent from client to server must stand on its own containing all necessary information required for the Server Toe Act. The client may store session state if it chooses. Http is a popular choice for most rest AP eyes, though it isn't required, given common operations like get post put delete along with intuitive status codes. Http pairs nicely with arrest interfaces. The presentation of resource is is often done through intuitive. Http. Your l's rest isn't the only option when designing a P eyes. So let's explore remote procedure Call or our PC based AP eyes. When you invoke in our PC operation, you call some local function with arguments. The function code then forms an AP I call to a remote device and executes a given action on that remote device. Fromthe coders perspective. It looks like a local function call, but behind the scenes, the procedure is executed remotely with rest. Resource is are modeled based on some intelligent and accessible hierarchy. Http or else are a popular choice for such exposure. With our PCs, you select your action and passing your parameters, and that's the end of it. You don't see or access. Resource is directly RPC AP eyes could also be transported in http. But occasionally we see other protocols in the network world. Net conscious, popular and is used for configuration management. Net cough is commonly transported in S S H. Both rest and our PC based AP Eyes can behave either synchronously or a synchronously. Let's explore these terms briefly synchronize AP Eyes are like two people having a conversation. The client speaks and waits for the server to reply, which could take a few milliseconds to several seconds. Asynchronous AP eyes enable the client to issue a request, then move on to other tasks, possibly issuing even more requests. The server will answer them later because synchronous AP eyes wait for the response you won't need multiple threads. Toe operate the program. Asynchronous AP eyes generally separate the request processing from the request fulfillment using multiple threads or other concurrency techniques. Synchronous AP eyes are easier to understand and implement, but generally perform in scale poorly by comparison. Asynchronous AP eyes need to somehow notify the clients that their request is complete so clients can update their current state. Asynchronous AP Eyes are a valid use case for the observer pattern recovered earlier in this course. If you're still not clear on the differences here, let me explain it. Using a fund analogy, it's getting late on a Friday and time to head home. The Global Man Tix sales team wants to take you out to the movies for all your great work. First, you arrive at the box office in the sales team, uses their bottomless budget to pay for tickets. They request tickets from the box office employees and then wait for the response containing their tickets. It's not like the box office employee asks them to step aside while she processes new orders. This is a synchronous exchange and doesn't include extra workers behind the scenes to fulfill the order before ducking into the movie, you want to grab some popcorn. This unique theater sells tacos instead, but you like tacos, so it's fine. The sales team places an order, and the concession stand employee gives them a receipt with an order number on it. This asynchronous operation allows both client and server to scale. The global Mantex team is then free to accomplish other tasks, perhaps getting a cup of coffee while they wait. The concession stand can take new orders while the taco makers fulfilled the existing order. Concurrently, both the client and server operate in a non blocking fashion. Another customer orders tacos before you receive yours. Many AP eyes operate this way as well. Using a job I d much like an order receipt. Finally, we receive our tacos. Once order number one is fulfilled and head into the show. Next, let's learn about http

Building Your Basic HTTP Knowledge
[Autogenerated] http Messages have three main components. Every http request contains some action such as get post put or delete upon receipt. The server processes this request and returns a response. These http messages also contain headers. Headers carry metadata about the device from which they are originated. Client request headers may contain browser details, while the server response headers may reveal the Web servers version. Headers can also carry details about authentication or payload formatting. Last The message body is an optional component that immediately follows the headers. Suppose you are uploading data to a server. That data would be included in the body. This data can be formatted in a variety of ways, such as Jason or XML, just to name a few. I want to briefly expand on the five common actions pertaining to rest a P. I's. The exact implementation of these actions will vary, but the general behavior is similar and http get is effectively a download operation pulling down a resource. This operation is typically read only and is used to collect information. The HD TV post creates new content such as adding a new resource as such running the same post operation in succession often leads to an error. Some AP eyes will create a duplicate resource with the different i d. In either case, this isn't desirable behavior. There are two ways to modify existing resource. Is, HTTP put is used to replace an existing resource in its entirety, such as replacing an existing device with an entirely new configuration. The put operation can be safely executed over and over. Unlike Post, you can also use http Patch something I seldom use personally. Toe update. Part of a resource. Not all. Arrest a P. I support it. The H C C P Delete removes an existing resource similar to a post message. You can't logically delete the same resource twice. Some AP eyes may ignore subsequent deletes, while others will signal an error beyond rest. AP Eyes. Http supports many other operations, such as Head Trace, Connect and Options. We won't dig into those, but it's good to be aware that they exist. I want to briefly explain what a Web hook is. A Web hook is a way for one app. To send an event based notification to another app using http we won't implement this, but imagine that one of global Mantex business customers. Wired Bring Coffee Company has an accounting system that it uses to track its revenues and expenses. Ideally, this system should show how much money wired bring coffee owes Global Man ticks at any time. Having the tool continuously poll The global Mantex CR M server is a waste of resource is also the accuracy of the data is dependent upon the frequency of the pole. Instead, the global Mantex app could send a Web hook, typically an http post, to the wired brain accounting app to provide real time updates whenever their balance changes. If wired brain made a $15 payment, their balance is reduced to $25 the Sierra map can report that this Web hook is sometimes called a reverse A p I or Web call back, No matter the operation. Http of responses always contain status codes. Let's quickly step through some of https more common status codes. The 100 Siri's messages are informational messages. For example, the code 100 is an interim response, which encourages the client to continue the 200. Siri's messages indicate success. The most common is 200 meaning okay, but to a one could be sent in response to a post indicating new content was created. 202 might come in response to an asynchronous post and contain a job i d in the body. The 300 Siri's messages indicate redirection 301 means a resource was permanently moved and often contains the updated You Earl. Alternatively, 302 indicates a temporary move. We've all seen these 400 Siri's messages in our browsers at some point, which indicate client errors. This could be due to failed authentication trying to access something for which we don't have permissions or a missing resource. Last the 500. Siri's messages represent server side errors. An internal air could mean a coding bug. Or perhaps we made a post request when the server doesn't implement that request type.

Analysis: HTTP GET and POST Packet Captures
[Autogenerated] This is a packet capture of the get request between my laptop and the Sierra map. These images come from a protocol analyzer called Wire Shark. I'm ignoring all of the networking details here because they aren't relevant. But we can see the protocol is http and that we issue a get request to the slash u r l. That just means the top most resource digging into the details. We see the entire http request expanded. The request line includes the get operation and it's followed by the headers. The host header is required and carries the U. R L String. The remaining headers provide metadata about my laptop browser. For example, I used fire Fox on a Mac book. In my language is set to American English. The headers in the body are separated by a blank line. Represented by backslash are backslash end as shown. This particular request actually has nobody, which is common. Forget requests. Since the client is trying to get something rather than send something, these square bracket wire shark lines are just comments not actual. Http. Data We'll see a really http body shortly. Here is the servers response. We see status code 200 which means okay, and the body is carrying HTML text. Looking at the headers, the server is telling the client that is carrying HTML text encoded using utf eight. It also tells the client there are 1973 bytes of data. The other headers provide additional data about the Web server. Notice the blank line again after the headers again. This separates the headers from the body. The body is large and I'm on lee showing the beginning of it. But you can clearly see the HTML text carried in this message. The browser will display this data by processing the HTML code. Suppose we punch in an account number into our app and click the submit button. What happens then? This generates an http post request from the client, which is carrying a web form. You can think of this as representing the data included in the input text field. As with all http requests there is ah host header specifying the you, Earl. We also see the usual headers that describe the client metadata and preferences. Check out these new content headers. The client is telling the server I'm sending you a Web form with 14 bytes of data. This tells the server how to process the http body. Here's that blank line again, which separates the headers from the body. The body contains what is effectively a key value pair where the key is account I D. And the value is account 100. Let's finish up by looking at the response from the Post request. Once again, we see code 200 indicating success. The headers remain almost identical to the previous http response. I'm adding a screenshot from within the body details to prove that the server actually does return the correct balance thes package captures are included in the course files if you want to dig deeper.

Introducing Cisco DevNet Capabilities and Resources
let me tell you about the incredible resource is that Cisco definite offers sandboxes are controlled environments where you can experiment with new products and technologies. Cisco offers reservation based and always on sandboxes, and many of the demos in this very coarse are backed by these sandboxes. At the time of this recording, I've contributed eight production grade code projects to the code exchange, and I encourage everyone else to participate as well. You can think of this like a curated collection of open source code. Definite offers a variety of tiered support options ranging from free community based forums and live chat to enterprise grade ticketing systems. This is a great place to ask questions and gain valuable advice. Augment your Pluralsight training. You can and should view definite specific training videos to learn more about the Cisco Solutions that interest you most. Devon. It also provides step by step instructions for their self paced labs. This is a great way to gain hands on experience. These labs are often integrated with the videos to create useful learning tracks. Last definite provides a handy of resource linking Cisco product, a p I documentation in a central place during our demos, I reference it very often

Demo: Cisco DevNet Virtual Tour
Let's navigate the definite website to explore all the resource is we just discussed. I'm currently at the Cisco Definite Home page at developer dot cisco dot com. I'm already logged in. It's free to sign up for an account, and I suggest you d'oh! I'm certain this Web page will change regularly, so I'm going to focus more on the capabilities than precise button clicking. Let's check out the video training first. Here is an introduction to network Program Ability course hosted by my friend and colleague Hank Preston at Cisco. This is just one video course of many, so let's go back in our browser. If we click to discover a menu item at the top, then look down to videos. We can see the full selection. I'll scroll down a bit just to show a handful. Here we can see videos on a wide variety of topics, such as Cisco Firepower Software to find whan umbrella and other new solutions. If you've never heard of these before, don't worry. I'll cover them in the future course. Next, I'll go back to the top click, discover and explore the sandboxes under code. We see sandbox remote labs. Let's explore those at the time of this recording. Sandboxes are broken down into technology areas such as networking data center, cloud, et cetera. Let's click on data center to see what kinds of sandboxes are available. Some of them, like this application centric infrastructure with kubernetes sandbox, require a reservation. This creates virtual infrastructure that is provisioned on demand. You can connect into Cisco's lab to access your sandbox. Other sandboxes are always on, which means you can just use them with no reservation. This a C I simulator is one such example. So let's explore it. This is the sandbox control panel. Let's scroll down on the instructions pain toe. Learn more because this is a public always on instance, the host name and credentials are public. This also comes with an expectation of professionalism, which means being a good citizen. As Cisco describes here. Let's go back to the definite home page by switching tabs, scrolling up and clicking the word definite. Next, let's check out community support options. This includes everything from technology specific support to general blog's to forums. Let's go to the forums. As you can see, there are a ton of discussions happening right now across many technology areas. We could drill into these different areas, but I'll just scroll down to show some of the recent topics. I don't think I need to explain how an Internet forum works, so I won't babble on needlessly. You should utilize these forms for any questions you have. Let's go back to the home page. If you are looking for a more structured learning approach, you can try out the learning tracks you confined these under discover these learning tracks, tire reading, video training and labs together into a coherent course for a given technology or solution. Let's scroll down and take a look at some of these offerings. Here's one on Cisco's digital network architecture. Let's check that one out. The path starts off with some network fundamentals past that it describes how to interact with DNA Center, a p I, which is precisely what we're going to do later in this module. The learning track advances to network program ability, a more advanced topic. I'll cover in a future course, but I think you get the point. Okay, back to the home page. I admit that I haven't found a great way to navigate to the A P I documentation using the Web view. But if you just slash docks to the end of the home page or L, it takes you to a well organized documentation page. This page contains a no nonsense list of links to relevant AP I documentation, something we will be using extensively in this course and future courses. Let's check out the digital network architecture or D n a center rest AP eyes using color codes. It clearly shows us the different http requests that we can send to DNA Center. This style of a P I documentation is known as swagger and is a popular approach to documenting AP eyes. Some products, including DNA Center, also host this documentation on box. So if you are operating in a private environment without access to the definite website, you can still access this information. Let's expand one of these AP I calls. This reveals extensive details about the operation, including return codes, the Jason structure of the body response and much more okay back to the home page again. Last, let's check out the code exchange located under discover. Cisco has recently added an ecosystem exchange which focuses on partner Integrations and Business solutions. But since I personally have published projects through the code exchange will explore that one scrolling down. We see several repositories representing different projects. I'll use the search bar to pull up one of mine. This tool is designed for service providers to automate their networks by managing customer multi tendency memberships. Let's look at the project details. Remember that. Read me. We did earlier in the course when learning about get up. That's what you're reading right now. In fact, this repositories hosted on Get Hub and using an AP I Cisco Devon, itjust reprints the information here my answerable and Python network automation courses. Here at Pluralsight, you dive into variance of this specific tool. This project served as the inspiration for both courses, so feel free to check those out That wraps up our tour of definite. Although I certainly did not do it. Justice explore on your own and I guarantee you'll be impressed

Introducing Cisco Digital Network Architecture (DNA) Center
[Autogenerated] Cisco DNA Center is a network management system with four main capabilities. The design capability allows operators toe hierarchically describe their network architecture by geographic region. It can manage software, images, device inventory and roll based templates. The policy capability helps enable Cisco's software to find access, or SD a solution. This integrates with Cisco security products we will discuss in a future course. In summary. It enables operators to clearly define the communication channels between systems. DNA Center can also provisioned devices on the network. I've personally used it to stand up virtual servers on a remote branch site using predefined images and templates. This can reduce costs and lead time associated with new site installations. DNA Center Assurance is network monitoring on steroids. It combines streaming telemetry and a variety of network management protocols, which provide a complete network health picture. It also provides suggestions to remediate faults. In the next clip, we will interact with DNA center using its rest. A P I

Demo: Constructing a REST API Call using Postman
[Autogenerated] The global Mantex network team is considering buying a management solution, but they have no A P I skills. Can you show them how to interact with Cisco DNA Center Before we dig into a postman? I want to show you the always on DNA center sandbox first, as we saw in the previous demo, this is the page you see, after clicking on a sandbox link, this sandbox is always on. So the public host name and credentials are displayed. We'll be using these to access DNA center Next, let me show you the AP i documentation, which explains the first few steps we should take in the definite virtual tour. We briefly skimmed the DNA center, a p I docks. I've scrolled down to the authentication access token Request section which displays a single http post request. I've already expanded the details. If we successfully authenticate, we should get back a Jason dictionary with one key named token. This access token will be used for a P I authorization for all subsequent requests. This obviates the need for supplying username, password credentials in every request. Going deeper, we see, http headers as indicated by the parameter type first we specify that we want Jason content. We also need to specify our log in information using http basic authentication For this, we can use the sandbox credentials shown earlier. Last if we get an http code for a one that means our credentials were invalid with this newfound knowledge. Let's jump overto postman. Next, you can download postman for free at get postman dot com and I'm showing the link in orange. This is an interactive, http client useful for communicating with unfamiliar rest ap eyes. I've pre made the request we need for this demo to save time, but we'll walk through them together first. We use http post to request an access token. I've specified the post method, but this drop down allows for other choices too. I also copied the host name from the sandbox and the resource u R l from the a p I docks to form a complete U. R l. In our case, we need to add to headers. One of them is credentials related, so let's check out the authorization tab. I've selected the basic off type. This is commonly used in http environments and allows us to specify a simple username, Password credential on the right. Just specify the user name and password from definite. Next, I'll advance to the headers. Here we can specify key value pairs for other headers such as content type. We can use the check box to enable or disable a specific header. And while postman doesn't show our authorization header here, it will be added. Since we configured authorization in the previous tab, we don't need to specify an agency to be body for this request. As the A P I docks don't require it, we can click the send button to test it out inside the body we see our token a long string, which is the value associated with the token key. This is precisely what the A P I docks said we would see recall that http responses have headers to the server is based on Engine X, for example, and did in fact provide us Jason content in reply. Okay, back to the token. I'm going to copy this into my clipboard in order to do something useful with this token. Let's go back to the A P. I docks at the top of the docks. We just skinned there is a critical sentence. It tells us that we need to include this token in an off token. Http. Header for all subsequent http requests. That's easy enough. We just need to create a header for those new requests while omitting the http basic off. Let's go back to Post Man and do it. Our next request is a get request that will return a list of devices from de Neck. So let's use the U. R L shown, which comes straight from the A P I docks. Now let's add our headers. And remember, we will not be adding http, basic off. This time we need to retain content type application. Jason. So the AP, I knows we want to use Jason. Then we add our token, which I'll quickly d'oh! Okay, let's send this get request. In response, we receive a Jason list of dictionaries. Each dictionary is a specific device, which contains a wealth of information about that device. In the next clip, let's follow this same process using a command line utility known as Curl

Demo: Quick and Dirty REST API Calls using curl
[Autogenerated] some of the network engineers don't have access to Postman. Is there a quick and dirty way to test a P eyes without it? Suppose you are using a deaf box without a graphical you. I like a tiny cloud computing instance. We can use Curl here, a simple multi protocol client application that comes with most Lennox distributions. And if not, it is easily installed using the OS package management commands shown. I'm also revealing my curl version for reference. The Man page for Curl is extremely long, so I'll just explain the commands as we explore them. The commands are very complex, so I've wrapped them into tiny shell scripts. These air files that contain bash commands that can be executed. And if you aren't familiar with this topic, I'll cover the details. In a future course, let's look at our get token dot s h file First, this is one large command. The trailing backslash is a multi line continuation character in bash, which allows you to break up a large command for readability. We begin with Curl, followed by Dash X, which allows us to specify the http operation. Remember, Our initial token request was an http post To specify the http basic off user name and password. We use the dash you option and pass in a string that contains user name colon password again using the credentials provided by the definite sandbox. Then we must specify the additional header off content type application. Jason, just as the docks suggest, I hope you are seeing the parallels between this text command and the graphical workflow in Postman Last, we specify the U. R L, which is the D N, a center host name, followed by the token Request Resource. We can run the script by proceeding it with a dot forward slash The DOT represents the current directory, allowing us to run this file as an execute, a ble, provided we have permission to do so. This just runs our curl command as if we typed it manually. Just like with postman, we receive a Jason dictionary back with the token. Ah, more complex script. Could programmatically pull out this token and use it for future requests automatically, But we'll use python for that in the next module. Let's explore one more curl example this time to get a list of devices. This time we are issuing a get request. Technically, I get request. Is the default for Curl? So we didn't need to say Dash acts get, But I like to be explicit. Now we have two headers toe add, and we use a separate dash H statement for each. We still want Jason Data, and we also provide our token. I added it behind the scenes to save some copy pasting. No more username, password needed. Here last, we have the Earl, which will provide us a list of network devices when we send the request. Let's run this script again. We have a long list of devices, each represented by a dictionary and each containing device specific data we may find useful. I've included The Postman Collection and Curl scripts in the exercise files for reference in the next module will repeat this using python code, but let's cover a few more AP I fundamentals first

Interpreting UML-based API Sequence Diagrams
[Autogenerated] unified modeling language, or UML, has many uses, but let's use it to create a P I sequenced diagrams for our serum app on the left, we depict the user. This can be done using a box containing text or a stick figure. A line representing time is drawn from top to bottom. The line is drawn as dotted and represents the lifeline of a given actor. In the sequence, we can repeat the process by drawing additional components to the right. I am drawing the M V C components in the sequence that they would be accessed when an AP I call is made. Ah, solid arrow is drawn from the originator of the request to the target. The client browses to the sea around happy, or l, which triggers an http, get the text associated with each arrow is meant to describe the action Upon receipt, the controller must process that get request. You might remember that our code had some conditional is to test for the type of http request received. Forget requests. The only action is to render the template containing HTML code and return it back to the view. This isn't a new A P I call but is a response to a previous one. So we use a dotted arrow. The view then crafts the http body taxed in html so the user's browser can display it again. This is reply traffic from the original get request, so we use a dotted arrow. Last we add activity boxes over the top of each participants lifeline The activity box identifies the subset of the lifeline where the component is active. This makes it easier to see one Components are being used and when they are not, these boxes do not have to be contiguous as each lifeline could contain many bursts of activity. Let's work through a more complex example what happens when the user enters an account number and hits Submit The user sends an http post request carrying the account i d to query the view doesn't process application logic. So it passes this information to the controller. The conditional logic for http post directs the Controller Tau asked the model for the balance of a specific account. I d. Using the balance method. The model could return one of two answers, and we depict this using an alternative box. This is a rectangle stretching over both components, with each alternative separated by a dotted line. Sometimes the word Ault appears in the upper left corner, but I'm omitting it here for cleanliness. The first alternative is that the account I D. Is valid and exists in the database. So the model returns the balance. The account I D could be invalid, in which case the model returns none. In both cases, the controller will render the template and display the proper output either a valid balance or an error message. This is carried back to the user inside oven. Http Response. Let's quickly draw our activity boxes to finish up.

Reviewing API Fundamentals
[Autogenerated] Let's button up our discussion on a P I. C. We discussed it several AP I types in this module, though we focus primarily unrest due to its popularity. Just be aware that other options do exist. Cisco Devon. It is a great way to learn at zero cost, and I'll be heavily using it for this course and many future courses. When it comes to issuing a P I calls, we can run ad hoc operations using Postman or curl to quickly test. Our logic in the next module will interact with Cisco D N a center using python code. See you there.

Deploying the Python requests Library
Module Introduction
[Autogenerated] this module would teach you how to interact with rest. AP eyes programmatically using python Here's the lineup for this module. The Requests Library is an intuitive and popular python library for interacting with Web servers. This is our focus for the next few clips, as this module is almost 100% demonstrations. In the previous module we used Postman and Curl Tau handled DNA center authentication. I'll show you how to do it programmatically as well. We'll continue into two challenging exercises. First, we'll collect a list of all devices that DNA Center is managing. Then we'll make changes by adding a new device, which has some hidden surprises. Last, I'll wrap up the course with a homework assignment and some final thoughts.

Demo: Authenticating to Cisco DNA Center via REST API
[Autogenerated] first things first, let's learn how to handle the multi step process of authenticating two d n a center using python. Think back to the previous module, where we used postman and curl to interactively grab an access token. We then use the token in subsequent http requests. To do this in Python will use their requests Library First, we need to install it with PIP before we explore the python code. Let's use the interactive Python interpreter toe. Learn what is available with the requests library. We could also scour the public documentation, but the request library is quite intuitive and a little bit of exploration goes a long way. First, let's import requests. If we want to see a list of attributes and methods that are available in this module, we can use the D. I. R function. Specifically, I want to call your attention to a few key methods, such as get post put, delete and request. I'm on Lee highlighting a few of them, But trust me, all of the different http methods air here For a more generic action, you can use requests dot request and then just specify which action you want to take but I prefer using the specific methods as it's a bit less typing. If we look at the dock page for the request method, it will reveal all the different minor options we can specify for any of our. HTTP requests. The required parameters are the method, which would be a string like get post put or delete. Also required is the U R L for the resource being acted upon. After that, there are many optional parameters. For example, we can specify a Jason body to include, perhaps in an, http post that adds a new network device. We can also specify. Http headers as key value payers in a dictionary much like Postman, we won't scrub this whole document as I'll explain these options as we encounter them last. I want to show you the functionality that is contained in the response object. When we issue a request, the A P I will return a response, and this response will carry information such as the body content and a status code. There are many attributes here, so let's put them into context by exploring the python code. Next, the first thing I'm doing is importing the requests module much like we did in the interpreter that will give us access to the HD to be client functionality we need I'm defining a get token method so we can reuse this code for our future challenges. Remember, functional decomposition is a pillar of good coating. I define three local variables just to simplify things. First, I define the AP I path, which uses the u R L provided by definite. Then I create a to tubal containing the user name and password. The request library will use this for http. Basic off during our http post to grab an access token. Last I define the required Headers Dictionary, which sets the content type to application. Jason, this workflow should remind you of postman and curl. With this data, we can issue our post request for an access token. I'm using the requests dot post method and passing in three parameters. First is an F string that takes our base a p I path and upends the specific resource we want to access. We don't want to repeat ourselves by typing the AP. I path over and over. I'm showing the full string and orange so you can see how it fits together, then we pass in our authentication tubal. Sometimes you'll see this implemented as shown in orange, which is really just a more explicit version to achieve the same result of using http basic off within request. Last we pass in our headers. Each request returns a response Object. I am calling this one off rest. We can pick this object apart to see what information came back before we drill into the response. Let's ensure the request succeeded. The rays for status method will raise an http error if the status code is 400 or greater, signaling a client or server error. If the http status code is less than 400 it does nothing. If you recall the response body is a Jason dictionary containing a single key named token with a capital T. We can convert the body text to Jason and then extract the token that will give us our token string, which I am returning to test our code. I've written a small main function. All I do is call, get token and print out the value. These last two lines in the script ensure that mane is called whenever this module is run directly from the shelf. When we import this Python module elsewhere, Main won't be called Let's execute this code and ensure we see a token printed out. Okay, that looks perfect. Let's modify our code by passing in the wrong credentials to ensure our failure. Logic works, too. I changed the user name to be Nick 123 So let's run it again. Now our script fails. The status code was for a one, a client error meaning unauthorized. The stack trace helps developers troubleshoot the problem. Let's move on to our first challenge that involves doing something a bit more interesting with the requests library.

Challenge 1: Get a List of Devices from Cisco DNA Center via REST API
[Autogenerated] this is an optional challenge. Should you choose to accept it, you can pause the clip and attempt at yourself or keep watching. If you want to enjoy the show, can you issue arrest a P? I call two D n a center to download a list of devices, then print out the device I P addresses and I d numbers. I'll give you two hints. First, I suggest consulting the definite AP I Documentation for DNA Center. You could also use the DNA center on box a P I documentation, if you prefer. Second, I suggest using postman to get the AP. I request correct before coating it up. This also allows you to observe the http response containing the body. You'll get back which you'll need to process upon reception. You can reference the clips in the previous module. If you need help. Good luck.

Demo: Challenge 1 Solution
[Autogenerated] were you able to solve it? If not, don't sweat. Let's walk through one potential solution. Okay, let me show you my solution in the get devices that p Y script. I'm importing requests again. But also they get token function from the off token module we wrote in the previous demo. This recycling of code will allow us to focus on the task at hand jumping into the main function. The first thing we do is request an access token. After that, I declare the AP I path and required headers. Now we have two headers, content type and ex off token. Remember, all future AP I calls will use this token and not the username. Password pair. Next, I send an http get request targeting the same u R L We tested with Postman and Curl to get a list of network devices. I don't expect anyone to remember what that complex Jason structure looks like. So I'm adding a bit of temporary code here. This will allow us to print the http response as Jason in a clean way, using the jayson dot dump string or dump ___ function. As a side note, you can put multiple lines of code on a single line using semi colon. But this is only appropriate when using debugging techniques. Let's run this code and see what we get. I scrolled up so we can see the top of the Jason structure, which is often important to learn how the data is formed. In this case, we have a dictionary with a key named Response, which has a list as its value. Therefore, we should be able to access this data by using the Jason function on the response object, then asking for the value at the response key as shown in Orange. We can see tons of information for this device, but we only want the management i, p address and I d. Here's an example of the management I P address and the unique I D, which is a unique string representing this device. I've updated our code behind the scenes to account for this, So let's check out those changes. Keep in mind that when we run this thea output will change regularly, since there are people from all over the world using this public sandbox. But this code will function correctly no matter how many devices there are the four loop steps over all dictionaries in the response list, then extracts the I D and management. I ___ into a single line of text. This code isn't http or a P I specific. Just basic python loops and data structures. If the request fails for any reason, I print out the status code and the air message, which might help us trouble shoot later. Let's test out this code. As expected, we get one line of summarized output for each device in the list returned by DNA center. Looks like there's only one device now, but that's going to change very soon. To recap how we did this, we first issued an http post to get an access token using http. Basic off. Then we use that token in an http get request to collect a list of DNA center devices. We can run this code as many times as we want with no human interaction needed. This is the power of a P I's and automation in general. Coming up next, we'll explore more complex A p I interactions

Challenge 2: Add a Device in Cisco DNA Center via REST API
[Autogenerated] Here's another challenge which build on our existing code. Can you figure out how to add a new device? Of course, these aren't really devices, but global Mantex maybe expanding their network soon and we'll want to update their inventory. My hints remain the same. Check the definite AP I docks for D N a center and see what kind of http request is needed to add a new device. Use postman to test your A P. I calls and feel free to use our existing python code to collect the list of devices after you add a device that's a quick way to verify the operation was successful.

Demo: Challenge 2 Solution
[Autogenerated] this one is tricky and requires a variety of different. Http requests. Let's check out how to tackle it. I'll show you how I solve this problem in the ad device dot p y script. I'm still importing requests and our get token function, but also including the Time Library. You'll see why shortly, Let's jump into the main function. Thes lines are identical to the previous challenge. We grab the token, then form our A P I Base Path and Headers Dictionary as a quick side note. If Cisco ever makes the always on sandbox, I'll read only resource. You can always reserve a sandbox with full administrative access to test this out. In order to add a new device, we build a dictionary that contains the information required by DNA Center. How did I get this information? You could get it from the product. A P I docks. However, we've already done that. So let's check out the Unbox D'Anna Center. Documentation for variety first, all log in using the same credentials we've been using for http Basic off. Once logged in, I'll navigate to platform first. Next we go to the developer tool kit. From here we can search for whatever operation we need, so I'll just scroll until I see an http post request to add a new device. Okay, I found it. Let's dig in. Just like the definite docks we see the u. R l a. The top, followed by more details about required information. What makes this request different than other request is that we need to supply in. Http body here is a model of how a data should be structured. The model identifies which parameters are required in which are optional. In my example, I included all the required parameters and a few optional ones just to demonstrate how it works. For example, we have to specify an i p address into cli transport mechanism, Which makes sense. However, other attributes relating to http are optional. Let's check out that Jason Schema. This shows us the required Jason structure of the http body. This is a basic dictionary without too much hierarchy. The only gotcha is the I P address, which is a list of strings. So we'll need to account for this. Let's get back to the code. Hopefully, this dictionary makes a bit more sense now I'm following the schema that the documentation provided along with my device specific values notice that the I p address key references, a list of strings, not just a string. Now we can issue R h d to be post message to add this device, we use the same your l and headers as we did for the get request, as directed by the A P I docks. Except now we specify in http body using the Jason keyword argument. This will take our python dictionary converted to the equivalent Jason syntax and carry it in the HD to be post request. When we issue this http post, we are not going to get a 200 status code back. This is an asynchronous operation. Unlike the get devices request we tested previously. Instead, DNA center will tell us. Okay, I'm processing your request. And if you want to pull me for status, you can use this task i d in an 80 tb, get request to collect additional info. Http. Status code to 02 is used to signal a successful asynchronous AP I response. Assuming the initial post succeeded, let's wait 10 seconds for DNA center to process the request. Then we'll issue another get request Using the task i d pulled out of the response from the previous http post. I'm showing that Jason response in orange so you can see how I came up with the corresponding python code. Remember my movie theater analogy? This task I d. Is like our taco order receipt. Once I have the task I d. I can issue a get request to a specific girl for a sink tasks. I just need to tack on the task I d to the end of it. I'm asking d n a center. You gave me this task I d. Is it done yet? Ah, more professional application might do this in a loop, but in my case, I'm expecting it to be done after 10 seconds as another alternative. Perhaps the server could tell the client when it's done using the observer pattern notify or a Web hook design. But we aren't using those advanced techniques here. The response from this request is shown in orange again so you can understand how I am accessing data. The two most important keys our progress and is error. The former will contain any human readable information often air messages, and it's sometimes blank when the operations succeed. The is Eric. He should be false, which indicates the operation succeeded. I print appropriate status messages based on this value. There are many, if else statements here at various levels just to account for the possible points of failure. Let's run the code and see what happens. Okay, First we see that the http post was accepted using status code to 02 Then after a 12th wait, we see that the device was successfully added. There were many opportunities for failure such as theeighty to be post failing the HC to be get to check on the task failing and the task containing some kind of internal error as to why the addition failed. Fortunately, none of those failures occurred. We could check the DNA center you I to ensure our device was added, but that's no fun. Instead, let's use our get devices script we wrote in the previous challenge. We should see an additional device now with I p address 1 92.0 dot to dot won a device we did not see in the previous demo. Okay, there's our new device with the proper I P address and a new, unique I D. At this point, you've seen examples of a C T B get and post operations usage of access tokens, synchronous AP eyes, asynchronous AP eyes and a handful of error checking techniques. As a side note I've captured example Jason dumps from these various AP I requests into the Jason Ref Directory. Feel free to use these as references. Having these handy can enhance learning and simplify troubleshooting. Let's wrap up the course in the next clip.

Final Challenge and Course Summary
[Autogenerated] as a homework assignment. See if you can build on what we've done so far by developing code to remove a device. I'll include the solution in the course files. This is a great opportunity to put your skills to the test. We've reached the end of the course. I want to share my final thoughts on our time together. This one is going to seem very obvious, but many programmers overlook it. Proper planning with respect to software design, including the use of design patterns and testing, almost always leads to a better outcome. Get is a topic that many find complex. It took me months to get comfortable with it. I strongly recommend sticking with it and constantly building your proficiency. Introduce it into all your projects and the basics will soon become second nature. If there is one thing you've taken away from AP eyes in this course, it should be that there is no right way to design or consume. And a P I. We've explored many types of AP eyes, along with some ways to communicate with them, both interactively and automatically. Always read the AP I documentation before diving in and you'll be in great shape. I want to offer my sincere gratitude to you for viewing this course. Be sure to check the course files for code samples, packet captures and more. Feel free to ask me questions on Twitter or in the course discussion at any time. Thank you.
