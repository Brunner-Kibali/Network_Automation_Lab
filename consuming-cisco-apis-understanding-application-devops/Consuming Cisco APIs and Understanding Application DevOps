Course Overview
Course Overview
Hi everyone. My name is Nick Russo, and welcome to my course on Consuming Cisco APIs and Understanding Application DevOps. This content is perfect for individuals who love hands-on examples and a practical application, but don't know where to start. In this course, we'll cover a variety of topics such as writing Python code to interact with popular Cisco product REST APIs, using software development kits or SDKs, exploring a variety of cloud and virtualization options, Docker container fundamentals and use cases, DevOps principles and continuous integration pipelines, and modern web application security. After completing this course, you'll be able to write, test, and maintain high quality Python API interaction scripts for whatever business project comes your way. Before beginning this course, I'd recommend the following prerequisite course. This Getting Started course covers important contextual topics surrounding software design, source control, and the construction of a simple web application that we use extensively in this course. You'll want to watch it first to best follow the business scenario. I hope you'll join me on this journey to learn about Consuming Cisco APIs and Understanding Application DevOps with this course at Pluralsight.

Leveraging Cisco Management Solutions and SDKs
Introductions and Prerequisites
Hi everyone. My name is Nick Russo, and welcome to another exciting Pluralsight course that digs deeper into the technical nuances of IT systems automation. This first module focuses on a pair of powerful Cisco management solutions. Let's dive in. Here's the lineup. This course makes some assumptions about the viewer's technical skills, and I like to explain these prerequisites early. Past that, there's this term going around called model driven programmability. I'll pull back the curtains and explain what this means. Next, we'll explore the REST API on Cisco Network Services Orchestrator or NSO. If you aren't a network engineer, don't worry. No network skills are required in this course. Last, we'll touch on Cisco's digital network architecture or DNA Center using a professional-grade software development kit or SDK. In the last course, we interacted with this API quite a lot, but wrote many custom scripts. This course attempts to tackle two different, but complementary topics. First, we will focus on how to consume APIs on a variety of Cisco products. While this is occurring, we are not focused on the Globomantics CRM app, but instead on writing Python code. Globomantics, our employer, is interested in evaluating many of these products for use in their infrastructure. Later in the course, we'll transition back to the CRM application to talk about DevOps and how we can apply those principles to improve the app. This CRM app is a big project in Globomantics, and our goal is to help build it. This course was designed to be viewed after completing Getting Started with Software Development using Cisco DevNet, an important prerequisite. That course covers the fundamentals of software design and API usage, as well as the Cisco DevNet resources available. I will be leveraging Cisco's DevNet sandboxes and API documentation extensively in this course, so you'll need to understand how to use them. The course also contains important scenario context about the Globomantics app we already started building.

Adding Structure with Model Driven Programmability
This clip will be short and conceptual since the heavy lifting on data modeling will come in a future course. Model driven programmability is a fancy way of saying that when we read and write information from our IT systems, that information should conform to some well-known structure. When I was in the military, we were required to pack our backpacks the exact same way. The shovel had to go here. Your eating utensils went there. Your clothing went somewhere else. This wasn't just for the sake of arbitrary discipline. It allowed troops to easily find items in the dark or when in a hurry. Data modeling is similar because it clearly defines when all the configurable items are, what their types are, and ultimately how they can be accessed. Another commonly used analogy is a government-issued ID card. Everyone has a name, which is a string. Everyone has an age and a weight, both integers. This information is always placed in the same spot on the card. Suppose we had a standard format for representing interfaces on our systems. Consider this JSON structure. If you want to update an interface, you'd need to supply an interface indexed by the name key inside of a dictionary along with other attributes. This dictionary would be appended to a list named interfaces. That interfaces list is contained within a dictionary called config. This model provides a standard mechanism to manage device interfaces. Imagine that all your Cisco devices follow this model. Rather than pushing shell commands, now you can manage infrastructure by modeling data against a common schema and filling in the device-specific values. These operations are also atomic, meaning they cannot be divided into smaller parts. Either the whole operation works or it fails, so you aren't left in an inconsistent middle state. Taking a step further, imagine that multiple venders or platforms follow this model. Now you can manage multivendor environments using a common model, which means lower maintenance costs. My code sample here is figurative since JSON is easy to understand, but the actual language used is YANG. Yet another next generation or YANG is commonly used for modeling data. YANG makes more sense when discussed in the context of network automation and infrastructure management, a topic that gets its own course. In that course, we'll dig deeper into YANG and build our own models.

Managing Networks with Cisco Network Services Orchestrator (NSO)
Cisco's Network Service Orchestrator or NSO solution is a perfect example of how to apply model driven programmability. The product is designed for network management. We learned about DNA Center in the previous course, which is focused more on enterprise networks with a handful of predefined use cases. NSO is more like an automation toolbox, which is both powerful and customizable. The cool thing about NSO is that the entire application is YANG-driven and as so has a configuration database or CBD, which is based entirely on YANG. All the devices in the network have their configuration state modeled in YANG regardless of how they are managed. Even ancient devices can be managed this way. Suppose you wanted to test NSO, but don't have a network for testing. NSO comes with a built-in network simulator where you can quickly spin up some test devices, add them to the CDB, and begin configuring them by making HTTP requests to NSO's REST API. That's exactly what we're going to do in the next few clips.

Demo: NSO Sandbox Quick Start Setup
At the time of this recording, Cisco DevNet does not host an always-on NSO sandbox. I'll rapidly set up the reservable sandbox instead. I don't want to dwell on installing NSO as this isn't an NSO product specialization course, so let's move quickly. I've reserved the NSO sandbox and received my time-based credentials from Cisco. The lab includes a Windows management station, a Linux DNS server, and NSO itself. From the Windows management machine, I've already logged in and grabbed the NSO host IP address, which is 198.18 .134 .28. Let's jump into the shell. I've also installed the openconnect package using the Linux package management commands shown. In this separate tab, I've connected our DevBox into Cisco dcloud where the NSO sandbox is hosted using the highlighted openconnect command. I should be able to ping the NSO IP to test it for reachability. I sent three pings, and they all succeeded. I'm going to use SSH or secure shell to connect into NSO in a third tab using the credentials shown so we can begin installation. The NSO installation guide is hosted on the Windows management machine and also in the DevNet documentation, and I've copied the key commands into a small shell script for reference. I'll enter the commands one at a time to explain each as we go, but please focus on the high-level flow, not memorizing commands. First, we want to install the NSO package that is present in the sandbox. Next, I'll use the source command to update my local shell with various aliases and binaries that are now available after the NSO install. Then, I use the newly available command, ncs-setup, to create a runtime directory for NSO called nsc-run. This next step isn't well-documented, but we need to copy the network element drivers or NEDs we intend to use from the NSO staging area into our ncs-run package location. For this demo, we are just using Cisco iOS, so I recursively copy the whole NED directory. Last, I switch into ncs-run directory and start NSO. Let's verify that NSO is running properly using the ncs--status command, and I'll use grep to cut down on the output. Okay, looks good. With NSO installed and running, we can now build a simulated network within NSO. Let me clear the screen for cleanliness. I wrote another script called build_netsim.sh to automate this process. The goal is to create three basic Cisco virtual routers. I use ncs-netsim create- device to add the first device named router1, and it uses the cisco-ios NED. I then add two more devices, router2 and router 3, to the virtual network using the add-device option. Then, I change into the netsim directory, start the simulation, and run a few verification commands. The is-alive command should report that all routers booted correctly, and the list command will provide some extra details about how to access each device. Let's run this script to provision our test network. Okay, everything looks good, and our test devices are running. Note the SSH port numbers of 10022, 10023, and 10024 for our three routers. Last, we must enter the NSO CLI and configure NSO to manage these devices. As a management system, NSO isn't yet aware that these new routers were added. I've captured the required commands in the netsim_config.txt file. After entering config mode, I define an auth group to enable device authentication. Next, I have three almost identical chunks of config. Each one defines a router accessible through the local host and using the specific SSH port numbers revealed by our build netsim script. We specify the cisco-ios NED, unlock the device object for editing, then commit the changes. The sync-from command is particularly interesting as it allows NSO to read the config state of the router into its database, allowing us to detect any config drift. I repeat the process for router2 and router3 just using different SSH port numbers and device names. Next, I'll enter the NSO CLI using the ncs_cli command shown. To save time, I'm going to copy/paste that config script into NSO. Let's check to ensure NSO is managing these devices correctly. Okay, this proves that NSO can see our new devices. In the next clip, let's issue some API calls against NSO to update these device configurations.

Demo: Device Management with NSO REST API
In addition to DNA Center, Globomantics has expressed interest in evaluating NSO for managing network devices based on YANG models. Let's jump right into the code. Our task is to get a list of devices, then add a new loopback interface on each router using the add_loopbacks.py script. I'll be using requests again to interact with the NSO REST API. Let's jump into the main function first. The basic api_path is shown next. Here's a sample from the NSO docs regarding the structure, and notice we used HTTP port 8080. The username/password is admin, admin, so we can define that in a two-tuple for HTTP basic_auth. Through trial and error, I discovered that we need four different data formats to be accepted. I define an accept list, then use the join function to combine each item together using a comma. This creates one long string with commas separating each value. Here's another quick documentation example of one of the accept types, and notice we are expecting YANG-structured data in JSON format for all resource types. Those headers were specific to get requests. And for post requests, we only need to specify YANG data in JSON format since this is what we will be uploading. Let's begin by collecting a list of devices. The API is structured according to the config hierarchy, similar to what we saw in the CLI. Here's a more specific example from the docs, but we can omit the details and just ask for a list of all devices instead. Then, I assert that HTTP code 200 is received in response. If there are no devices, that means we didn't build our simulated network properly, which returns an HTTP code 204, meaning no content. So I raise an HTTPError. Assuming we did get a list of devices, let's pull out the device list from the JSON output. I'm showing a subset of that structure in orange, and you can reference the course files for more details. Then, we can iterate over this list of devices. Each device has a JSON hierarchy of parameters that follow a YANG model, which we'll explore more in a future course. Here's a quick look at this hierarchy, and the tailf-ned-cisco-ios string is the specific YANG model used for the interface structure. Once we get this list of existing loopbacks, I want to transform these dictionaries into strings by accessing the values and appending them to a list. The resulting list should look like the orange callout as an example. I'm throwing in some print statements to print basic stats on each device, such as the device name, IP address, SSH port used, and current loopbacks. All this information is printed on a single line for cleanliness. Next, I define a dictionary to add a new loopback where the interface number will be equal to the SSH port. Note that I specify which YANG model I want to use so NSO knows how my HTTP body should be structured. Router1 will add loopback 10022, router2 will add loopback 10023, etc. Once the dictionary is built, I issue an HTTP post request that carries the new loopback and targets the interface configuration URL. The URL changes with each device since we are adding a new loopback to each device individually. If the post succeeds, I print a message stating so. If it fails, I do nothing. Most of the time, it will fail because the interface already exists. So I'm taking a different approach with error-checking just for variety. Let's run this code and examine the result. For each device, we see a line of output printing the basic information, and notice that each device only has loopback 0 to start. Each device also had a new loopback added, so let's run this script to view our device list again. Now we no longer see the message about new loopbacks being added, but each device has a new and unique loopback interface already. The number is equal to the SSH port number assigned to each device as expected. In the next few clips, let's learn about an alternative to writing custom API scripts.

Demo: A Bigger NSO Sandbox
[Autogenerated] this clip is a new addition to the coarse. In the previous demos, we installed NSO and built a small simulated network. This new definite sandbox is much easier to use and comes pre configured with many devices. So let's check it out. I'm at the main page for this new sandbox. Judging by the topology, Cisco is not messing around. There are a lot of devices here across many platforms. I've scrolled down to the log in details so you can see how to connect and will be using this data in our scripts. Like the other NSO sandbox. This one also requires a reservation and I've got it reserved for this demo. Let's head to the Dev box to get started. I've added a directory named New NSO, which contains our new code. Let's go there first since we've already explored how to collect and manage interfaces using the NSO rest a p I. I wanted to mix things up by doing something different. This time will collect the AARP table from multiple devices and aggregate the results into a C S V file. To accomplish that, I've written the get AARP stats Stop P Y script. We begin by importing that requests library just like last time. Additionally, I've included the item getter class from the operator module. I'll explain how this works later when it becomes relevant. Let's move into the main function next. To keep things simple, I've recycled some of the code from the previous NSO script. This will allow us to focus on the new and interesting components rather than rehash the basics you already know. The A P I path is different in this sandbox and notice the rest. Cough slash data text will discuss rest cop in much more detail in the next course. But in short, this is how you interact with Yang. Model data over arrest, a p I. It's newer and more commonly used than the proprietary rest api I we explored in the previous demo. But both AP eyes are supported because this sandbox uses https with self signed certificates. Let's ignore any validation warnings to keep our output clean will define a to topple with our user name and password for http. Basic off. Just like last time. This information came from the main sandbox page we saw earlier. We still need to accept the same data formats as last time. You can trim this list based on your specific use cases, but to be safe, I'll include all of them here. This approach allows you to add additional requests later without needing to troubleshoot. Http headers unnecessarily using the joint function, we combine all the strings in the list together with commas in between. Since we are writing the AARP entries to A. C S V. File, let's include the column Names for readability. The AARP table string defines those columns separated by commas and ending with a new line. Wilken Captain eight individual AARP entries to this string as we discover them. Next, we'll enter a four loop that iterating over three devices. The 1st 2 are legitimate devices representing the two distribution routers in the topology, which run IOS XY. The third is a non existent device, but unlike last time, we won't _____ the program on failure. We'll just skip that device, which is an alternative approach to handling errors. First, let's define a device specific u R l This adds some additional text onto the base. You are l B to find earlier and targets the specific device in each loop Federation. In addition to the sandbox, Definite has also added NSO documentation, which I've linked in the read Be. Here's a screenshot showing where I learned how to form my http requests, because we're targeting our data. Let's extend the Earl even further by only checking for the AARP entries in the live status container. If you look at the full device dictionary shown in the call out, you'll see that it is separated into multiple components such as config, live status and more. I have included samples of these outputs in the data ref directory, so these call outs are just subsets for brevity. We'll also pass in our http user name, password pair, headers and SSL. Verify settings like last time. We expect a status code of 200 as anything else is a failure. If a failure occurs, will print a message for the failed device and continue looping to the next device. Assuming the http request succeeded, let's extract the AARP entry list. Here's a call out showing that structure in greater detail and weaken. Dig into the dictionary by indexing the two keys specified Notice that each entry is a dictionary with a handful of key value pairs. If you'd like to see these Jason dumps in real time, you can uncommon this debug statement to explore them. Next, let's iterated over that list of dictionaries. Accept will sort it first. The item get her class that we imported makes it easy to sort. A list of dictionaries based on the value at a specific key in this example will sort based on the interface value, meaning that gigabit Ethernet to should appear before gigabit Ethernet three and so on. Inside the list will can Katyn eight. A new table row containing the device name I P address, Mac address interface and optional age in minutes. If the age is not specified, will substitute Ah, hyphen using the dicked dot get function. At this point, the AARP table string is a large multi line string containing our comma separated data. Let's write it to a file named AARP stats dot c. S V Using a single right operation. This is slightly faster than executing many rights within a loop. Such optimization doesn't matter for our small lab, but in real life it can be significant. Last, let's print a useful message to help users read the file. CS fees can be sloppy to read, but the column command makes it easier. As you'll soon see let's test our script. Using the Python Command shown, we see two lines of output. First, the script could not collect AARP stats from the bogus device, which is expected. The word bogus should be completely absent from our final see SV file. We see the recommended column command, so let's use it. I'm going to pipe the output to head toe Onley. Reveal 15 lines as this file can get pretty long now we see the global AARP Entries on the two distribution devices. Notice that for each device the entries are sorted by interface, thanks to our usage of item getter. As a small challenge, see if you can extend my code to include the VF aware AARP entries as well, which are mapped to gigabit Ethernet. One. That's a good way to gain NSO experience in the new definite sandbox while also improving your python and a P I skills

						Why Are Software Development Kits (SDK) Useful?

By now, we've written many scripts from scratch to interact with APIs. Is there an easier way? The main purpose of a software development kit or SDK is to simplify the process of writing code for a specific project. For example, imagine being able to create a DNA Center object, then using premade methods to list, add, and remove devices. 

That's certainly easier than our approach in the last course. Also central to many SDKs is the concept of abstraction. Rather than manually crafting HTTP requests with custom headers and authentication tokens, it would be easier to call a method named login, for example. Good SDKs allow programmers to focus on solving business problems rather than focusing on low-level interactions. Most professional-grade products have publicly available SDKs. 

When you are first learning, starting with SDKs can lower the barrier for entry and allow junior programmers to get going quickly before they interact with the API directly. In our case, we started with the custom scripts to get foundational skills first, but either approach is fine.

						Demo: From Custom Scripts to the DNA Center SDK:

Globomantics wants us to explore using a newly published DNA Center SDK instead of our homemade scripts from the previous course. Can we integrate this new SDK? First, we need to install the SDK using pip. The command is pip install dnacentersdk. Okay, now we have the SDK ready to go. 

In the previous course, we wrote Python scripts to get a list of devices and to add a new device. Let's first explore how to get a list of devices using this SDK. I'm importing the API module from the dnacentersdk package early on. This API functionality obviates the need for issuing manual HTTP requests. Let's check out the main function next. Using the api.DNACenterAPI constructor, I create a new DNACenterAPI object by passing in our sandboxed hostname and credentials. This single statement collects an access token and handles all relevant error checking. 

In the interest of time, I won't browse the docs for every API call or SDK method as you can easily access those. I'll include snapshots of the relevant docs instead. After that, we can get a list of devices simply by using the highlighted method. This will return the body of the HTTP get in JSON format, which is what we use to manually do. Checking the docs, we can see optional parameters, such as device filters limits whether the output should be sorted and more. 

In our case, let's just grab all devices since there aren't very many. Notice the commented out debugging code. In the previous course, we've used this technique to progressively build our solutions. But in the interest of time, I won't uncomment them here. Instead, I'll use callouts to show the JSON structure and supply JSON examples in the course files for your reference. 

Specifically, this device list should look familiar as it's the same output we saw in the previous course, except this time it required far less effort to obtain. I simply copy/pasted the for loop from our preexisting Python scripts here. I step through each device in the list stored under the response key and print out the unique ID and management IP address. 

Let's run this script and ensure it works. Just like last time, we get one line of output per device. And right now, there are two devices being managed. Let's add a new device using another simple Python script, again leveraging the SDK. These two scripts are similar, so I'll focus on the differences to avoid repetition. 

We now import the time module, which I'll justify shortly. We still create our DNACenterAPI object using the same credentials, which is roughly equivalent to the GetToken helper function from our custom scripts. Then, we define a dictionary to add a new device. This dictionary contains the same data as the one used in the previous course. 

So we don't have to dig through the docs again to figure out how to structure it. However, we do need to know which method to call to add a device, and I'm showing a screenshot of the docs for that. We can use add_device and pass in our parameters piecemeal, although I prefer a different approach. Using the double asterisks in this way, we can unpack a dictionary into keyword arguments. This allows us to carry a portable collection of our method arguments and expand it when making the call. 

Note that we are adding a device with IP address 192.0 .2 .1 using an HTTP post behind the scenes. The response from this add operation is asynchronous. And to keep things simple, I use logic similar to our preexisting scripts next. I wait 10 seconds, giving DNA Center time to fully process the addition. Then, I extract the taskId from the JSON structure. Here's an example of that structure, and we can query DNA Center for a status update using the taskId. 

To accomplish that, we use the get_task_by_id method and pass in that taskId, which relies on an HTTP get. The docs suggest that this simple method doesn't have many options, and we only need to supply the taskId. That method returns the task status formatted as JSON, and we will need to extract the isError value to ensure it is false. This if/else statement prints either a success message or a failure message. And if failure occurs, extra detail surrounding the failure reason are printed. Let's run this script and see if it adds our new device. 

To verify that our new device was successfully added, let's use our device getter again. Perfect. We see 192.0 .2 .1 in the output, indicating that the device was successfully added. To summarize this clip, the value of SDKs is that they simplify and abstract API operations. Just a reminder that the output from the commented out JSON debugging statements are captured in these files for your reference. We'll explore more Cisco APIs in the next module.

								Module Review:

This module was designed to introduce model driven programmability and SDK consumption along with some real-life Cisco management solutions. The whole idea of using prescriptive data modeling languages like YANG is to improve data standardization. This often extends across vendor boundaries, providing a uniformed model for programmers to consume. 

As we'll see in a future course on network programmability, mastering YANG takes practice and specialized tooling. The goal of this module is conceptual understanding. Data modeling is becoming so central to modern IT automation that entire products are built off it, such as Cisco's NSO. This product warrants and entire course on its own, but I demonstrated some of its power by utilizing our existing REST API skills to manage a simulated network. 

Last, if an SDK is available, consider trying it before you interact directly with a device API, especially if you are writing production code. Some APIs are relatively easy to use even without SDKs, such as DNA Center and NSO. However, the benefits of extensive testing, documentation, and error checking are key for production solutions, and SDKs usually bring all three. In the next module, we'll explore many more Cisco products. See you there.

						Exploring Cisco Purpose-built Products:

Introduction and Important Notes

So far, we've explored some Cisco management solutions like DNA Center and NSO. Cisco has many other offerings across every IT vertical, so let's dive in. I've broken this module into five major parts, which separate the IT infrastructure into discrete domains. First, I'll introduce Meraki, a cloud-managed infrastructure solution for managing wired and wireless networking. Next up is software-defined WAN, a modern solution to reduce transport costs by efficiently using all available network paths. In the data center, we'll learn about Cisco offerings to solve computing and networking problems, which includes management solutions. Collaboration continues to grow in importance as teleworking becomes more common, and Cisco has several innovative approaches to improve team cohesion. As networks grow in size and importance, security becomes ever more critical. I'll finish up by touching on Cisco's broad, but tightly integrated security portfolio. I have three quick, comments before we begin. I'm not here to sale Cisco products. I'll explain the capabilities with no fluff and summarize how the solutions tie into the larger IT architecture. I'll also be demonstrating basic API interaction because this is a programming- oriented course. You don't need networking expertise to understand this content. Just focus on the business problems and how the proposed solution addresses them. Don't get wrapped up in the network details, but rather stay focused on the capabilities and APIs. We are going to cover about 15 different Cisco products in this module, some of which have multiple APIs. Detailed demonstrations for all of them would require multiple dedicated courses. I'll demonstrate the flagship products, but provide code samples for many of the others, which you can access from the course files.

Managing Wireless Networks with Cloud-based Meraki:

Globomantics has a growing wireless network as employees frequently bring their own devices to work. Managing this network has become increasingly complicated, and they've asked us to explore Cisco's Meraki solution. One thing that differentiates Meraki from traditional wireless networking solutions is that it is 100% cloud managed. The Meraki hardware components, such as wireless access points or wired switches, all need to reach back to the Meraki cloud over the internet. This provides centralized management and control so that IT staff don't need to deploy heavyweight servers to locally manage the network. Once deployed, both wired and wireless clients can connect to the network and access corporate resources like email, file sharing, and more. These clients could be end users or even servers in a small data center. The Meraki management service is accessible to IT engineers through a dashboard or APIs. Because the solution is cloud-based, employees can easily manage the system from anywhere, whether at work, at home, or on travel. Now that we understand the high-level design, let's touch on a few key Meraki capabilities. Meraki is typically managed in two ways, either through a web-based dashboard or through its REST API. Our focus in on the REST API, but you do have options. Because the solution is centralized, operators can see all devices in the network, including client devices through a single pane of glass. You've probably heard the term single point of failure in a negative context and looked for ways to eliminate them. Meraki handles all that automatically behind the scenes, providing high availability without customer interaction. In terms of deployed hardware high availability, Meraki has a variety of features to support that. The ability to roam between sites on a campus is critically important for multi- site enterprises. Meraki handles this seamlessly. And, in some cases, it works even if Meraki hardware cannot talk to the Meraki cloud. Along with roaming is enrollment, allowing both bring your own device users and corporate clients to rapidly onboard. In terms of endpoint management, Meraki can also centrally deploy apps to smartphones and other devices. Whether you haven Apple or Android-based device, Meraki can help manage those apps. For example, our spiffy new CRM app could end up on corporate Globomantics smartphones using this capability.

Demo: Collecting Meraki Network Details Using REST API
As a growing company, Globomantics intends to manage a solution like Meraki primarily through its API. To keep research costs down, let's explore the Meraki public DevNet sandbox. Let's explore the Meraki API. This is the always-on sandbox, and I'm highlighting the public API key we can use. The username and password are only needed if you want to check out the user interface. Let's switch to the shell. We are going to do three successive actions in the get_devices script. First, we will get a list of all organizations. Then, we will get all the networks within the DevNet organization. Last, we'll find the DevNet test network and list all of the Meraki devices inside it. We still import requests for HTTP client operations. More interestingly, do you remember in the previous course where I discussed the three pillars of good coding? Those habits weave their way into these demos too. The Meraki get function is an example of functional decomposition. It takes a string resource as an argument and is designed to simplify issuing HTTP get requests. I'm defining two variables, the base api_path and the headers dictionary. Per the Meraki docs, we need to use the X- Cisco-Meraki-API-Key header with a value of the public API key from the sandbox page. Here's a snapshot of what the HTTP request might look like, which shows us the required headers. Next, we can issue an HTTP get request by combining our API path with the resource passed in. We also include our HTTP headers so authentication succeeds. If the get request fails, we'll raise an HTTP error. This occurs when the status code is 400 or greater. Assuming the request succeeded, we'll return the JSON body back to the caller. Now let's explore the main function. We immediately employ our Meraki get helper function by getting a list of organizations. The full URL is shown in orange, which is the base api_path plus the organization's string. You might be wondering how I knew to use the string organizations. This was outlined in the API documentation shown here in the snapshot. Next, we need to do two things together. Let's print out all the organizations one per line. We use a basic for loop to step over all the organizations returned, printing each one. This:less than 6 formatting is just a whitespace alignment technique to keep our Name column aligned to the left. We also try to find the organization that contains the word DevNet and if so store the organization ID in the devnet_id variable. Assuming we were able to find the DevNet organization ID, we can get all the networks contained in that organization using our helper function again. This time, we pass in our organization ID along with the word networks, indicating we want a list of networks returned. Here's a documentation snapshot for reference, and you'll notice these URLs are pretty intuitive. Let's use a similar strategy to search the networks for the DevNet-specific network as there are many other test networks present. We'll print out each network while also checking to see if the word DevNet appears inside. If it does, we'll store that network ID, which is a long string. Here is an example JSON block containing a few networks, which is returned by our helper function. Iterating over this list of dictionaries and accessing data is relatively easy. Assuming we found the DevNet network, we can finally get a list of Meraki hardware devices inside that network. Peeking at the documentation, we feed in the dynamically discovered networkId, then add devices after. We'll use a simple for loop to print them out and include the model ID and LAN-side IP address. Here is the JSON structure of the device list that comes back from the Meraki get function. I like the Meraki API because the data structures are generally flat and simple. Let's run the script. Remember that the output will vary regularly as this is a public resource. Scrolling up, we see several organizations, and one of them is the DevNet sandbox with a unique ID. Our code searches the list and finds it, then gets all the networks inside of that organization. Then the code discovered the DevNet Always On Read Only network and asks for all the devices inside of that network ID. We end up with two devices. These are specific Meraki products currently in use within this DevNet Read Only network. In the next few clips, let's look at another technology used for connecting remote devices.

Optimizing WAN Performance with Cisco SD-WAN
Globomantics really likes Meraki's feature set, but this alone doesn't address the wide area network or WAN transport problems. The existing Globomantics WAN connects remote sites back to main headquarters typically across large distances. In general, SD-WAN solutions seek to optimize WAN bandwidth usage by selecting the most appropriate transport based on the application's performance requirements. Email and file transfer services require high bandwidth, but can tolerate latency, jitter, and loss. Real-time interactive traffic, such as voice, cannot tolerate these network effects, but doesn't require much bandwidth either. SD-WAN allows the network fabric to make intelligent decisions on a per-application basis, which is enforced through centralized policy. Unlike Meraki, this solution can be hosted in the cloud or in a private data center, making it a good choice for highly regulated organizations. SD-WAN separates the exchange of routing information from the flow of data itself. The remote devices, called WAN edges, communicate back to these smart controllers to receive their networking routing instructions in a hub-spoke fashion. The actual flow of data is different, flowing directly between two WAN edges to create an optimal path. Last, the vManage component is similar to Meraki's cloud management component, which has a dashboard and a useful RESTful API. This is responsible for interfacing with the SD-WAN fabric. SD-WAN brings a lot to the table and is projected to be a major disruption to traditional WANs. Like Meraki, SD-WAN has a central point of management, sporting a web-based dashboard and a REST API. Also like Meraki, the single pane of glass is where operators configure their desired traffic forwarding policies. Think of an apartment complex. If you've ever lived in one, you probably didn't know all your neighbors even though you were co-located. You were segmented into a building with a shared roof, parking lot, plumbing, etc. The same concept applies in networking as we may have different organizations physically at one site, but we want to keep their data separate. This improves security and manageability. For additional protection, SD-WAN includes intrusion prevention capabilities. This gives fine-grained control over which applications are allowed to talk to which other applications, improving the security posture of the whole organization. If you have thousands of remote sites, you want to plug in your gear, walk away, and have it just work. Using an auxiliary component known as vBond, WAN edges can discover their vSmart controllers. vBond serves as a broker to marry up these SD-WAN components to form the fabric with minimal effort.

Demo: Gathering SD-WAN Site Information Using REST API
Globomantics management thinks this is a feasible solution to optimize their existing WAN transports, ultimately reducing costs. Let's explore the API to understand better. Let's start with the SD-WAN public sandbox. Once again, we see the credentials publically displayed for our consumption along with the vManage hostname. Take note that there are four WAN edges, one vBond, one vSmart, and one vManage. So our script should find all seven devices. Let's check out the code. This script is also called get_devices, except we won't need to do quite as much exploration as we did with Meraki. I don't define any helper functions here since we aren't issuing multiple similar HTTP requests, so let's jump right in to the main function. The API path is the URL we saw from the DevNet page. Some of these sandboxes use self-signed SSL certificates, so I am ignoring any obnoxious warnings. This will keep our focus on APIs and not testbed distractions. The documentation snippet displayed tells us to create a dictionary with two keys, j_username and j_password. I'll copy that code and just modify the values based on the DevNet sandbox credentials. In the last course, I said that REST APIs are stateless from the server's perspective. Clients can optionally maintain session state, and the requests library gives us this capability using the session method. It returns a session object that we can run HTTP requests against. The SD-WAN API actually requires this, and we can use our usual HTTP request methods on it. Here, I issue a post request to the j_security_check URL. I pass in our credentials dictionary as the HTTP body and disable SSL validation. I knew this was the correct URL to use because it was contained in the SD-WAN API documentation shown here. Interestingly, this API returns status 200 even if authentication fails. To know if it truly worked or not, we need to check the HTTP body text. If the body contains any text, then the authentication request failed. So I print an appropriate message and gracefully exist the program. A response containing a non-error HTTP status code and an empty body indicates success. Once that's done, we get full access to the API without additional tokens supplied in headers. This is one advantage of client state retention. Next, I issue an HTTP get request to collect a list of devices. Since I'm accessing platform data, I use the dataservice string in the URL, which isn't necessary for initial authentication. The rest of the script should look familiar. If the device response came back without errors, I convert the body to JSON and extract the value from the data key. Remember, you can check the JSON ref directory in the course files to see the full JSON output, but here's a sample. Last, I use a simple for loop to step over the list of dictionaries and print out the device IP and device name using left- aligned columns. I hope you are seeing a pattern between these API demos. Let's test out this code. As expected, we see our four WAN edges, one vManage, one vSmart, and one vBond. Note that vEdge is the old name for WAN edge. They mean the same thing. Let's move out of the WAN and into the data center in the next few clips.

Exploring Cisco Data Center Solutions
When it comes to the data center, Cisco has countless solutions, both hardware and software. We'll focus on four products in this clip. First, we have the Unified Computing System or UCS Manager. This product is designed for managing Cisco computing hardware, such as rack and blade servers inside of a private data center. It can also manage Cisco's popular hyperconverged solution known as HyperFlex. The product enables administrators to apply server policies in a centralized way. UCS Manager has an RPC-based XML API that describes methods for programmers to interact with the system. Cisco Intersight is a cloud-based Software as a Service offering for Cisco compute management. Like Meraki, it comes with built-in high availability and unlimited scale. Intersight can also leverage information from other customers. For example, if another company uses Intersight and their data center crashes due to a bug, Intersight can notify all other tenants, such as Globomantics. Sometimes it can even take proactive action to remediate these faults. Intersight supports a modern REST API for programmatic access. UCS Director has a somewhat misleading name because it does much more than just manage Cisco UCS platforms. It's comparable to DNA Center except focused on the data center. It can interact with many other vendors, such as F5 load balancers, VMware vCenter, NetApp storage applications, and many more. It is excellent for initial provisioning of new services, as well as workflow orchestration. Last, UCS Director has a REST API with extensive documentation. Another popular Cisco data center product is application-centric infrastructure or ACI. ACI is somewhat similar to SD-WAN in that it manages an entire collection of network devices as a single application-aware fabric. ACI is based on the Cisco Nexus 9000 hardware platform, a data center networking device. Here's a sample connectivity diagram of how they might be interconnected. If you remember the SD-WAN vSmart controllers, ACI uses an application policy infrastructure controller or APIC, which serves a similar purpose. It communicates to all the network devices in the fabric and controls how they forward traffic within the data center. Imagine distributed applications that follow the MVC design pattern discussed in the previous course. Large-scale apps would probably have different components spread across different racks that need to communicate. ACI can enforce policies to ensure the view never talks directly to the model or vice versa. Additionally, ACI can govern what traffic is allowed to enter and exit the data center at the interconnect points with the rest of the enterprise network. Users should be able to access applications, but not manage the fabric directly as only the APIC administrator should be doing that. Let's explore four big capabilities of ACI. Centralized management is a recurring theme. The industry is demanding solutions that have nice-looking dashboards plus strong API support. ACI has a REST API, which we'll be exploring in the next clip. ACI introduces a number of new network concepts, but I'll explain two key ones. An endpoint group or EPG is a collection of hosts with similar security attributes. A contract defines the communications that are allowed between two EPGs. This is a segmentation technique to ensure EPGs only exchange authorized traffic, improving the organization's security posture. Initially, ACI was limited to a single data center, but has since been expanded to reflect modern enterprise computing requirements. ACI can be extended across multiple sites to enable increased availability and resilience. Much like SD-WAN, the ACI fabric is plug and play. The fabric can be physically interconnected with the APIC hanging off at the edge. The APIC will automatically begin discovering all nodes in the fabric. Once complete, the APIC will manage the fabric, allowing administrators to configure application-specific policies.

Demo: Viewing ACI End Point Groups (EPGs) Using REST API
Globomantics wants to explore a new data center architecture that would allow them to deploy and manage distributed applications with ease. To do that, let's explore the ACI REST API at a basic level. I'll start in the ACI simulator sandbox. Because real ACI deployments are typically expensive and hardware-based, this simulator gives us easy access to the API. I'm highlighting the API target hostname and required credentials, which we'll need to include in our script. Let's jump over to the shell to begin. To mix things up, we'll use the ACI REST API to collect a list of endpoint groups or EPGs, which is a concept specific to ACI. Let's check out the code. Like most scripts, we import the requests library first, but let's quickly move into the main function. The api_path here uses the URL from the sandbox along with the string api at the end, which I'll explain in a moment. Notice that we have a nested dictionary with a topmost key of aaaUser. It's a similar process to a DNA Center, except with ACI, we must include our credentials in the HTTP body and not using a basic authentication header. This particular ACI instance is using a self-signed SSL certificate, so let's ignore any warnings. Next, we issue our HTTP post request carrying the body of our credentials dictionary defined above. The URL and body format are described in the documentation. This JSON should be the body of an HTTP post request so we can get a token. Like many API calls seen in previous demos, we want to raise HTTP errors if we received an error in response. And if not, let's convert the HTTP body into JSON. After that, we need to dig into the complex JSON structure that is returned from the HTTP post request. Here's a quick look at the JSON structure, which requires traversal on several collections to extract the token. Given this token, we can assemble our HTTP headers for all subsequent requests. The documentation doesn't appear to provide a code example, but does provide a textual explanation here. We need to identify the token as a cookie using the specific format shown. Once that's done, we can issue an HTTP get request to get the list of EPGs. The documentation gives a more complex example using a specific query filter. But if we truncate those details, we can collect a list of all EPGs. Just like with the HTTP post for authentication, we will raise HTTP errors if this get request fails. If not, we will parse JSON from the response body text. Here is an example of the JSON structure we might get back. We need to iterate over the list contained in the imdata key, then dig a few layers deeper to access important data. In this case, I'm printing the distinguished name on a single line for each EPG. I try to keep these loops simple and consistent between examples. Let's test out the script. Looks like we had 22 total EPGs with 1 printed on each line. Cisco and other third parties have developed SDKs for ACI as well, and I'd suggest you take a look at those if you're interested. Next, let's talk about how to automate Cisco collaboration products.

Collaboration Using Cisco Unified Communications (UC) Solutions
Next, let's explore four popular Cisco collaboration offerings. Cisco has a collection of products known as Webex devices, which include smartboards, video teleconference endpoints, and tablets. These are used for daily collaborative activities in companies large and small. These products support a REST API called xAPI, apply named because it only supports XML payloads. Unified Communications Manager or UCM provides call control logic for Cisco's voiceover IP technology. This product succeeds the legacy Post Branch Exchange or PBX phone systems of yesteryear. It offers two APIs. The first is for user data services or UDS and is REST-based using XML data in the payload. It also supports a simple object access protocol or SOAP interface. This is an XML-based protocol to exchange data over HTTP. Finesse is a product best explained with a business example. Suppose you are a supervisor working at a call center. What kind of information would you want to know from the attendants in your charge? Average call time, average wait time, break schedules, and possibly more. Finesse provides all this, as well as many tools for the call attendants. Like the other products, it supports an XML-based REST API for programmatic access. It also has a JavaScript API for adding gadgets, which are many web pages that appear on the Finesse dashboard. Webex Teams is Cisco's Unified Communications platform. It supports instant messaging, presence, video, white boarding, file integrations, plus developer features, like bot creation and markdown rendering. You could compare it to Slack. It has a powerful REST API that also supports JSON. Let's dig deeper into this solution in the upcoming demo next.

Demo: Posting Chat Messages in Webex Teams using REST API
What if the Globomantics CRM app could post reminder messages into Webex Teams whenever accounts are past due? Let's write a conceptual example for our development team. With Webex Teams, we don't have a sandbox per se, so I'm at the API Getting Started page. There are many ways to access the API, but I'll use the simplest method of getting a 12-hour access token. I've already logged in using my personal email account, which is free. By clicking this copy link, I can copy the token to my clipboard for use in my script. As a reminder, this is for testing only, not production use. Let's go to the shell next. I'll blindly run the code without adding my token to show you a quick example of good error checking. For variety, I'm raising a value error with a useful message rather than just printing output and existing gracefully. Also for variety, I want to use environment variables here, which is a topic we covered in the Bash demo from the previous course. According to the message, I need to export an environment variable named WT_API_TOKEN, and the script will read the value for authentication. Now the script should work. But before running it, let's explore the code. I need to import both the os and requests libraries here. We need os to handle our environment variable. I define a few constants early on. I want to post a specific message in the Globomantics teams room, also called a space, and I want it to put these at the top of the script for easy editing. You could also read them from a file or from CLI arguments. The API URL is shown next. Webex Teams used to be called Spark, and I expect this URL to change soon. But for now, let's use what works. Here's an example API call from the Webex Teams documentation that collects the current rooms, revealing the base API URL. Then I read in the value from our token environment variable. The dictionary_get function will return the value of the specified key if it exists or none otherwise. The os.environ is a dictionary containing environment variables. If we didn't define a token, let's raise a value error, which is what I demonstrated earlier in the clip. Assuming we did define a token, we assemble our HTTP headers and include the bearer_token as the value. Here is an example from the documentation that shows a curl example for reference in case you'd like an example. Then, we use the get_rooms request to list all the rooms that we are currently in. If you created a new account, you can create some new rooms through the UI as I did for testing. Again, here is the API URL, which we saw earlier when building our base URL. This code should look familiar by now. If the request failed, we raise an HTTPError. Otherwise, convert the body text to JSON. Next, we use a simple for loop to search for the room ID for the room named Globomantics. Unlike Meraki, we are not printing out all the rooms this time, but looking for a match and then existing the loop with the break statement. Here is an example of what the JSON might look like, and you can see the reference files I included for more detail. If we found the room, we need to build an HTTP body representing the message to post into the room. The docs show us the URL and supporting arguments, such as the roomId. We can also specify the text message we want to post, which was the constant we defined earlier. Given this information, we can build our request using the proper URL, headers, and body. As always, I perform our standard error checking to ensure the request succeeded. At this point, our message has been posted, but I print a quick log to the console, just logging the text posted and who posted it. This information comes back from the HTTP post, which is a synchronous operation, and here's a JSON example. Let's run the script again. Okay, it found the Globomantics room and reveals the very long roomId. Then, it shows the text message that was posted along with my account's email address. Let's check the Webex Teams UI to confirm it worked. You can download the Webex Teams app, which is what I used. Or you can browse to the URL displayed in a web browser. Also, please ignore the previous test messages as I modified the demo message to be unique, which proves that the HTTP post worked. Let's round out this module by discussing some Cisco security products next.

Protecting Networks with Ciscos Next-generation Security Products
The days of standalone security boxes have ended, and Cisco brings a robust set of products to address all aspects of digital security. Let's discuss the most modern ones. Firepower combines next generation firewall and intrusion prevention into a centrally managed system. You can describe fine- grained security policies somewhat similar to ACI except in a general network context. You can think of this system like a locked door of a house where only certain traffic is allowed to pass through. This can be deployed as a standalone device or as part of a centralized architecture. The identity services engine provides authentication and authorization services to clients in both wired and wireless networks. It can also provide device onboarding and client security posture assessments. This is like the bouncer who guards the door to ensure no one sneaks in. Everyone who does gain entry gets a pat down too. Suppose you're not at home and not behind the safety of your locked door. Umbrella is a secure gateway solution that provides a line of defense against internet threats. Umbrella uses the domain name system or DNS to redirect traffic into the cloud for protection from malicious sites. We'll cover DNS basics in a future course, but you can think of Umbrella like a secure roaming technology. Advanced malware protection or AMP comes in two flavors, for endpoints and for networks. For endpoints, the application is installed on client devices, which allows it to perform malware scans using Cisco's threat intelligent database called Telos. For networks, the solution is similar except it ties into appliances, such as Firepower. Firepower informs AMP regarding any suspected malware, and AMP provides a risk assessment in response. Last, we have Threat Grid. This enhances AMP by providing a detonation chamber sometimes called a malware sandbox. Suspected malware is executed in this secure environment, revealing details about its behavior. This entire process is fully automated and tightly integrated, significantly improving overall security. I haven't mentioned APIs for individual products because all of these solutions support modern REST APIs, making it very easy to access them programmatically. Be sure to check the course files for some examples.

Module Review
Let's recap what we learned in this module. We've reviewed the capabilities of several products servicing the campus, WAN, data center, collaboration, and security domains. We also walked through some basic Python demonstrations for some of these products to interact with their APIs. It can seem overwhelming to understand all the nuances for such a diverse set of products. Fortunately, that isn't what this course is about. Instead, I'll challenge you to find the similarities between the APIs. In my opinion, the biggest difference between them is how authentication is handled. The usage of the Python requests library for most products was quite similar. I've included well-commented code samples illustrating REST API consumption for a handful of these products. This includes the products we didn't explicitly demonstrate here. Be sure to explore those files to improve the depth of your understanding. In the next module, we'll discuss real-life application deployment, containerization, and more. See you there.

Designing Application Deployments in Various Environments
Introducing Different Types of Application Environments
Just as important as learning to consume APIs and write code is understanding how to deploy applications. There are many important design tenants here, so let's unpack what this really means. This module is a good mix of theory and hands-on demos. We need to start by discussing the manner in which applications should be deployed. Should we use the cloud or a private data center or maybe both? Once we answer that question, we want to get more granular and determine whether we want to virtualize our app and if so what technology to use. We'll finish up the module with an introduction to Docker. We'll be focused on three main topics. Once I finish the high-level explanation, we'll explore a Docker file, which is used to create Docker images. Specifically, we'll be dockerizing our CRM app from the previous course into a single container for simplicity. As an added bonus, I'll introduce you to the docker-compose utility, an alternative to using shell-based Docker commands. Globomantics isn't quite sure where this new CRM app should be deployed. They have private data centers, but what other options exist? One popular choice is using the public cloud. Large cloud service providers like Amazon Web Services or AWS, Microsoft Azure, and Google Compute Platform or GCP are all viable options. In short, Globomantics can pay a usage-based fee for consuming cloud resources, then access their application from the remote cloud data center. Alternatively, Globomantics could deploy the application locally. There is a difference between private cloud and a classic private data center. The former implies that there is some kind of abstraction and orchestration to help manage the environment, something like OpenStack. Some engineers also consider Cisco ACI to be a private cloud solution. A third option is hybrid cloud, effectively tying the Globomantics private cloud into a public cloud. This is a common technique to leverage public cloud resources for some tasks, perhaps those that are compute-intensive, while keeping sensitive or regulatory data within the confines of a closely held site. These three cloud solutions generally focus on centralization to varying degrees. While this can reduce cost and complexity, it also puts the applications farther away from the users who access them. Edge computing is a distributed design that puts smaller computing capabilities closer to users like at branch sites or intelliworkers' homes. Let's dig a little deeper into these options, first by comparing public and private cloud. Most public cloud providers use a consumption-based, pay-as-you-grow billing model. It's similar to your electricity or water bill. And if you are efficient and conservative, you can reduce your overall expenses. If you already have a private data center environment, you can leverage those existing assets for cloud. You'll need to retrofit your data center with cloud software, something like OpenStack. Imagine you live in an apartment unit. Are you responsible for the foundation, roof, or plumbing? Probably not. Just like in public cloud, the cloud service provider handles the infrastructure management to include compute, network, storage, and environmental components. You just show up and start deploying your software just like moving into an apartment. Private clouds give you much more control over how the infrastructure is set up. Most public clouds have limitations on what kinds of traffic they allow to traverse their fabrics. But in private cloud, you make all those choices. This comes at a cost as you are personally responsible for maintaining the infrastructure, similar to owning a home versus renting. This is a quotation from a colleague of mine who is a security professional. While public clouds often have excellent cybersecurity defenses, you never really know where your data is stored. Some industries, such as payment card information and health care, have tight regulations on how data is stored, processed, and transferred. You can run apps with controlled data in the public cloud, but be sure to follow the rules. Naturally, if you control the facility and all the equipment inside, you'll have a much more accurate picture of where your data resides. Make no mistake, attackers can still breach your private cloud defenses and steal your data. I am only saying that at least you know where your data is supposed to be. In some industries, government regulations discourage the use of public cloud entirely. The promise of public cloud is unlimited scale. You don't need to know or care about how many servers are in Amazon's US East Coast regional data centers. Just keep deploying more instances. Of course, there is a physical limit, but through abstraction and good IT service management, the threshold should never be reached. Any privately built architecture, cloud or otherwise, is going to require a sizeable investment. Scaling up or scaling out will also cost you. Worst of all, you may not even use all the resources you purchase, which is not a concern in public cloud. You'll need to meticulously plan everything.

Alternative Environments: Hybrid Cloud and Edge Computing
As you can probably guess by the name, hybrid cloud tries to maximize value by focusing on the strengths of each strategy. Given that Globomantics already has private data centers, it would make sense to utilize those assets. They can augment their existing infrastructure by tying into the public cloud rather than growing their private sites. In addition to just scaling, this also provides new capabilities. As a personal example, I enjoy using Amazon's developer services like CodeBuild, CodeCommit, and CodePipeline. Such services could be handy for developers at Globomantics as they work on the CRM app regardless of where the app actually gets deployed. The number 1 benefit is flexibility. If you make the wrong choice by deploying the CRM app to the public cloud when it really should've gone into the private cloud, just move it. If you didn't buy enough servers for your private cloud to host your existing apps, move some of them to the public cloud. This adaptability helps overcome planning and execution inaccuracies. Edge computing is a completely different paradigm for app development. This is a distributed compute design whereby rather than centralizing massively scalable data centers in a small number of locations, we distribute less powerful computers to a large number of locations. Your city's fire department is a centralized shared resource, like a cloud computing environment. Placing a fire extinguisher in every home is like an edge computing environment. There is endless debate about whether cloud or edge computing is more reliable, secure, cost effective, or scalable. Rather than debate these topics, I'll state an undisputed fact that edge computing is faster. By putting apps closer to users, we substantially reduce the time it takes for users to interact with those apps. If you ever tried to access services on another continent, you've probably experienced unpleasant delays. The best use case for edge computing is to support the Internet of Things or IoT. Think about wearable electronics, autonomous vehicles, smart streetlights, and other things that aren't traditional IT endpoints. In some industrial applications, such as manufacturing, mining, and medical surgery, machines need to operate with precision. The IoT-enabled instruments need to talk back to their controller, which if hosted at the edge is very close by. This shortens the command and response cycle. Other applications include branch offices or tactical military networks where access back to main headquarters is not always guaranteed. Next, let's talk about how we can virtualize our apps should we decide to.

Exploring Different Virtualization Technologies
In a previous clip, we discussed potential locations for our application. We still haven't helped Globomantics decide how to deploy it. The traditional option is to deploy one application on one compute platform. You would have a physical box for your email server, another for your chat server, another for your fileshare. This is known as deploying to bare metal as there is no virtualization layer. It's a simple, but expensive option since there are often unused resources on each host. Bare metal is a good choice when you need extensive processing power, such as databases, gaming machines, or graphics processing. Also, some legacy applications cannot be virtualized and must run on bare metal. A very popular approach over the past several years has been using virtual machines or VMs. Instead of installing apps directly on the server, you would install a virtualization layer known as a hypervisor. Then, you would create individual VMs to run inside the hypervisor to represent your apps. This improves efficiency by turning the server from a large single-family home into an apartment complex with many units. One drawback of this approach is performance. Many VMs contending for the same resources means that engineers will need to allocate CPU, memory, disk, and network resources carefully. Another virtualization technique is to use containers. Rather than install a hypervisor onto a server, you would install a single operating system, let's say Linux. Next, you'd install a container engine, such as Docker, which gives you the ability to build containers. Containers share the underlying hardware, operating system kernel, and many of the system binaries. These shared resources are read-only, and the app-specific components are encapsulated in a container. As such, containers are very small in size and often boot in seconds. Let's explore the container architecture using a simple diagram. At the base of any solution lies the underlying computing hardware, like a server. Somewhat similar to a bare metal installation, an operating system is installed onto the server, such as Linux, Apple OS X, or Windows. In this example, I'll stick with Linux since we are primarily concerned with Docker containers in this course. Next, we have the container engine. This is installed like a regular Linux package and is supported on common distributions like Red Hat, CentOS, Ubuntu, and more. Installation only takes a few minutes. Now this platform is capable of running Docker containers. Each individual container could be a standalone app, a component of a distributed app, or just about anything else. These containers will rely on the underlying Linux kernel, which means only one kernel to patch and upgrade. This means you could use CentOS to host Docker, then run Ubuntu containers on top. All these Linux distributions share a common kernel, and that is what Docker shares between the host operating system and the containers. The containers can run application-specific code and packages as needed. Let's apply this design to our CRM app. Separating our CRM app components this way is too technically challenging for a beginners course. But conceptually, this should make sense. In the MVC design pattern, we can scale out model, view, and controller components using containers. If the app is distributed with each component encased in a container, this design would work nicely.

Demo: Building a Dockerfile
Globomantics management thinks that containerizing our CRM app is the right long-term approach. Even if the app remains monolithic, we can containerize it using Docker. Before diving into our first Docker file, I'm revealing the contents of the directory. The src directory contains our MVC app source code that hasn't been changed in a while. The setup directory contains some important Bash scripts. I'll discuss Bash scripts more in a future course. But for now, just think of them like a list of commands without any complex programming logic. Right now, I have one script, and let's explore it very briefly. This script automates the installation of Docker on CentOS, and I bet it works on any Red Hat-based distribution. First, the official documentation suggests removing any existing Docker packages. Then, I install a variety of supplementary packages followed by adding a specific Docker community edition or CE repository. After this repo has been added, yum will search it for any Docker packages, which occurs next. Once installed, we use the systemctl command to enable Docker to start when CentOS boots and manually start Docker immediately. I print the Docker version for confirmation, then give the user an optional command to install a dummy container as a test. The docs suggest we'll need to use sudo or sudu on CentOS, which isn't a big deal. Let's run this script, and I'll fast- forward through the install, which takes about 60 seconds. Okay, everything worked. Let's run this dummy hello-world container using the command shown. Lots of output, but it's basically telling us that everything succeeded. That hello-world container was assembled using a Docker file, which is how you describe what comprises a Docker image. Let's check out the Docker file used to containerize the Globomantics CRM app. The hash symbol is a comment, and I use them extensively. Docker files must begin with the FROM command, which specifies a base image. This can be OS-specific, language-specific, etc. In our case, we want to ensure the container has Python 3.7 .3 as this was used to develop our app. The alpine variant is a lightweight and minimal Linux distribution, making it an excellent choice for many containerized apps. Containers should be kept as small as possible for efficiency and portability. The callout provides a URL on Docker Hub where these base images come from. So feel free to browse. The LABEL command allows you to specify useful metadata. In my case, I'm specifying the maintainer attribute and putting in my personal email address. The legacy maintainer syntax is shown in orange, but I'm trying to illustrate the modern approach. The RUN command lets you execute arbitrary show commands inside the container. Often this is used to install packages. I don't need to install any Linux packages, but do need to install flask to run our CRM app. Often in Docker, you'll see a minimization of run commands by executing really long Bash statements instead. Each RUN command adds a layer to the container that increases the build time and image size. Layers are a nuanced Docker topic we won't dig into here. But the callout shows a common approach for running multiple commands. Imagine you're inside the container, and you've just installed flask. You now need to run the app. We need to switch into the src directory so we can access the start.py file. We use the WORKDIR command for this. The EXPOSE command enables external network access to a container. It doesn't necessarily change the Linux OS hosting Docker, but instead provides a hook into the container. If you remember, our flask app runs an HTTP server on TCP port 5000. Let's present that to the Linux OS. We'll explore how to publish a public- facing port soon. If you don't know much about networking or TCP, don't sweat. That's not too important right now. Last, we use a combination of the ENTRYPOINT and CMD commands to start the program. Using CMD in conjunction with ENTRYPOINT means that the CMD items are appended as command line arguments to the ENTRYPOINT binary. This executes the command Python start.py, which is exactly what we want. Now that we've seen the Docker file, let's build and run it in the next clip.

Demo: Using Bash Commands to Deploy Docker Containers
Now that we've covered the Docker file, let's transform it into a useable Docker container. Before building the CRM app image, let's list what images we already have using docker image ls. So far, only the hello-world image is present. This was automatically downloaded when we ran this test container after installation. When we build our CRM app, we will have two more images. One will be the Python 3.7 .3 base image, and the other will be our newly built CRM app. We can use the docker image build. command to start a build using the Docker file in the current directory. The --tag option allows us to specify an image name. Often this name:version format is used to differentiate between versions of the same container, and latest is a convention to indicate the newest version. Let's scroll up and review the output. First, the build process pulls down the Python 3.7 .3 base image, which is step 1 of 7. Next, we add our label, a relatively basic task. Now the fun starts when we install flask, and the output looks just like what we've seen before. All the dependent Python packages are automatically installed too. Once complete, the remaining four steps are executed, which includes switching to the src directory where code is stored, exposing TCP port 5000 for external network access, and specifying how to run the container when we start it. Let's check our image list again. Now we have three containers, and it looks like our CRM app only added about 10MB onto the base container. That's pretty small. Next, let's run the container. I'll use the docker container run command with a few options. Because it's such a long command, I split it up over a few lines using backslash. First, I am mapping TCP port 5001 on the public side to TCP port 5000 on the container side using --publish. This means we can access our container by connecting to our Linux box using port 5001, which gets forwarded to the container on port 5000. The next parameter introduces Docker-persistent data options. There are two types, volumes and bind mounts. A volume is a standalone datastore accessible to a container, and a bind mount is a path in the Docker host that we want to be made accessible to the container. I'm opting for a bind mount here. Remember, we need our source code to be accessible inside the container or else Python start.py will fail. Using some variable substitution, we take the current working directory and append src, then map this to /src inside the container. Now when workdir tries to change into the /src directory, it will work, and all the code and back end accounting data will be available. Last, I specify the target of the command, which is the name of the image we want to run. Okay, the app starts like usual, so let's try to access it from our browser using port 5001. Okay, this looks good. Let's try out account 100 real quick. Looks like it works as we see a valid balance. Let's go back to the shell and stop the container. We can use Ctrl+C to stop it, then docker container ls-a to list the container, including stopped ones. I realize this output is ugly due to line-wrapping. But at the top, we see our CRM app with a long container ID. It exited with code 0, meaning no errors. Let's quickly delete all these old containers using docker container rm following by the first few digits of each container ID. The container ID abbreviations we enter are reflected back when the removal succeeds. Suppose we want to deploy more containers while also avoiding that complex Bash command. I'll teach you how in the next clip.

Demo: Saving Time with docker-compose
The docker-compose utility is a quick and easy way to stand up Docker containers without complex shell commands. I'll give you a quick demo on how it works. Docker-compose is a handy program that can assemble and run containers on a single host. It removes the manual actions of building and running containers and allows us to define the sequence of events in a YAML file. This improves portability and also allows us to version control our container infrastructure. First, we have to install it. I've added another Bash script in the setup directory, so let's check it out. These commands are copied right from the official install guide, so I don't want to turn this into an advanced Bash lesson. The first command uses curl to download the docker-compose file for our specific operating system. These uname commands run locally and collect information about our platforms, so curl can find the right version of the binary. On my system, those commands return the string shown, making the entire string docker-compose-Linux-x86_64. Then I move the binary to the proper location as specified in the docs. Last, I ensure the binary is executable by adding the execute permission using +x. Again, some of these Bash techniques are beyond the basics, but let's focus on Docker here. I finish up by printing out the docker-compose version. Let's run the install script. It runs very quickly, and we see that the install succeeded as the docker-compose version command shows a valid version. Let's check out the docker- compose.yml file next. At the top of the file, we specify a docker-compose formatting version. Sometimes the exact format changes. And since we installed Docker version 19.3 .1 earlier, we should use docker- compose format version 3.7. Docker's website has a nice table to guide you in selecting a version, and I'm showing some of the entries here. More importantly, the services key specifies the containers we want to run. My first container is named crm1. The build subkey with a value of. specifies that we should use the local Docker file to build a new image. The image key sets a tag on this image, and I'm calling it crm:latest. The ports key specifies a list of strings, and the format is similar to the publish option seen in the previous clip. TCP port 5001 is publically accessible, and TCP port 5000 is privately exposed on the container. Docker handles the port forwarding behind the scenes. Last, I specify my volumes, which is a list of dictionaries. Specifically, I want to use a bind mount mapping the local src directory to the remote/src directory. This gives the container access to our local source code. I want to spin up two more containers using the same image perhaps for future scalability. Crm2 doesn't need to rebuild the image, but instead can just use the crm:latest image we already built above. The only difference is that I'm exposing TCP port 5002 for the second container. You cannot map two containers to the same public port because then Docker wouldn't know how to forward HTTP requests. Both containers can listen on port 5000 locally, which is inherent to our flask app, but the public ports must differ. Then, for crm3, it's identical to crm2, except I used TCP port 5003 instead. Basically, I'm creating three instances of the CRM app. To run this file, we can simply say docker-compose up. We didn't need to rebuild our Python or CRM app images. But if these images ever got deleted from our image list, docker-compose would rebuild them. It immediately creates all three containers, then starts their flask apps. The output from each app is piped back to the shell with different colors to make differentiating the output easy. Let's open a web browser with three tabs to test all three. Notice I'm connected to all three containers. Let's issue three separate post requests for three different accounts, then check our logs to ensure they function as expected. On the first container, I'll ask about account 100. Okay, we see a $40 balance as expected. Now on crm2, let's try account 200. This customer has a balance of -$ 10.00, which also looks correct. On crm3, I'll enter a bogus account of nick123. As expected, it fails. Let's check the shell output next. Hopefully the color codes make this easy, but now we can see all those requests. Here we see the account 100 request on crm1. Here is the account 200 request we did on crm2. Last, here is the invalid request tested on crm3. We can stop the containers using Ctrl+C again. If we look at our container list, those containers are still here although stopped. Deleting them manually would be a chore, so let's use docker-compose down to automatically clean up. See how easy that was? I'm a huge fan of docker-compose. And if you want to see a practical application, stick around for the next module on DevOps.

Module Review
This module focused on application deployment and all the considerations related to doing it right. First, we discussed different strategies for deploying apps in a variety of cloud and edge environments. I've personally used each of these environments to varying degrees. They are all uniquely useful given specific business requirements. In terms of virtualization, we may choose virtual machines, lighter weight containers, or choose not to virtualize at all. Much like the cloud deployment strategies, each one of these decisions makes sense in different scenarios. Last, I demonstrated using Docker to containerize the existing Globomantics CRM app. We discussed the basics of a Docker file, deploying images locally with Docker, and using docker-compose for some basic container orchestration. In the next module, I'll introduce the term DevOps, and we'll apply those principles to our CRM app. Stay tuned.

Getting Familiar with DevOps Processes and Tools
Dispelling the Buzzword "DevOps"
DevOps is a new way of approaching how we build, operate, and deliver IT services. This naturally includes some practical applications for Globomantics. Let's jump in. This is one of my favorite topics because the skills are both practical and powerful. I like to think of DevOps in four main components with several derivative principles, so we'll start by discussing those. Continuous integration and continuous deployment or CICD is a core component of process automation within DevOps. I'll also discuss test-driven development or TDD, a new way of writing software. Next, I'll briefly explain linting, unit tests, and system tests. Then we'll implement each one as it relates to our CRM app. We'll wrap up by exploring a real CI pipeline using Travis CI to tie in all the tests we created. To me, DevOps consists of four deeply integrated, but unique, components. Purpose has to come first. Have you ever rushed into a coding project with only a vague notion of what you were building only to dig yourself into a metaphorical hole? Or maybe you built something no one wanted or needed. You must understand why you are writing code and what business problem is being addressed. Next, there is a social component to DevOps, which involves people tearing down silos between departments. In short, this means a reduction of turf wars and politics between teams with an increase in information sharing. A process is a sequence of steps that yields a desired outcome. Any deviation from that desired outcome can be considered a defect. Through technology, DevOps seeks to engineer the perfect process, which is continuously improved to reduce cost and delivery time for app deployments. When we discuss CICD pipelines, that's an example of a DevOps process. Last, the technological tools. This tends to be the most interesting for technical people because there is such a wide selection out there today. I'll touch on a few in this module as we apply DevOps principles to our CRM app. Because this is a technical course, this module focuses mostly on the process and tool components. While purpose and people are critically important, I can't teach you how to code that, though I'll add commentary about these components wherever I can. Given the main components, we can get a bit more specific around what DevOps is. The opinions run far and wide here, so I'll keep my thoughts brief. First, the concept of quality at the source. Suppose you work in a manufacturing assembly line. If your upstream workstation handed you a component that was obviously defective, would you shrug it off and include it in the final product? I would hope not. The idea of quality at the source is to introduce testing early in the build process, sometimes at every stage, to ensure bugs are caught sooner. Continuous incremental improvements are much preferred over large-batch gambles. Think back to the Kanban workflow process from the previous course. We want continuous value delivery with future deliveries guided by customer feedback, market demand, and other proactive measures. This last one captures two important thoughts in one pithy phrase. Unite through failure implies that the team grows stronger as new ideas fail and new learning is generated. This failure should be embraced and dissected with blameless postmortem analyses. The unification of responsibility across the entire value stream from concept to delivery is reinforced by shared consciousness. I'll wrap up by dispelling a few myths about DevOps. DevOps does not mean every developer becomes an operations engineer or vice versa. The goal is to merge the development and operations functions into a single coherent chain of progressive events with smooth handoffs. DevOps is not a product. I once supported a customer who built a system consisting of network devices, servers, and a laptop. They called it a DevOps kit. This customer made no effort to clarify their purpose, reform their culture, improve their processes, or even introduce new tools. They callously shoved it into production, and it was a complete failure. Simply applying automated testing or consuming REST APIs or deploying code every 34 seconds does not mean you are doing DevOps. To achieve that, you'll need to abide by the four foundational components outlined earlier. True DevOps means delivering continuous customer value through team work and integrated processes.

Introducing CI/CD Pipelines and Test-driven Development (TDD)
Pipelines are best described with a simple flow diagram. Here is an example using the Globomantics CRM app. We talked about DevOps processes before, which is a sequence of steps that yields some result. In our deployment process, the first step might be a quick syntax check of all source code. There is no point in running code that is missing a curly brace or has undefined variables. This is often called linting. Once that's done, we run unit tests, which focus on testing one specific method or class. Think about assembling furniture. The instructions always ask you to check all your hardware ahead of time to ensure nothing is missing or damaged before you start assembling the product. Next, system tests tie multiple units together. Maybe you've build that new rocking chair, and you give it a good shake to make sure it isn't loose. You are testing the product as a complete system. If the chair shakes, the test fails, and you can remediate. The last step is acceptance testing. The chair is complete, but how do you know if it works? Try sitting in it. Acceptance testing validates that the app does what the user expects it to do. Often this includes automating web browser operations or perhaps a bit of manual testing. I won't demonstrate this, but frameworks like Selenium are often used for web app acceptance tests. Assuming all of these tests pass, your code is automatically deployed to production. After all, your code is only as good as your tests. So given strong test coverage, this is a great strategy. However, I'm certain some of you tensed up a little at this idea. What if you aren't quite so confident and would prefer to deploy manually? Check out this workflow. All the automated testing steps are the same, except deployment happens at the click of a button by a person. How's that sound? This is where we differentiate between continuous delivery and continuous deployment. Regrettably, both begin with a d, which makes the CD acronym ambiguous. The later automates the entire deployment chain, while the former automates all testing, packaging, and staging. A human retains control over production deployment, and this is a good approach to gain confidence. Note that all the testing phases can be summed up as continuous integration. Simply doing CI without any kind of CD is still a powerful approach and a great first step. Speaking of testing, some developers prefer to author their tests first, a methodology known as test-driven development or TDD. If you write your test first, you are guaranteed to have 100% test coverage. Naturally, the test will fail until the subject under test is properly implemented. This helps drive better code quality since you'll know with certainty when you are done with implementation. This also helps ensure that all code in your project is actually being used. Those with software backgrounds have probably seen legacy code in projects that should've been removed, but was frozen there for whatever reason. Those with network backgrounds have probably seen bogus configs sitting in network devices for ages. I've seen both, and it's simply wasteful. TDD reduces this because no one is going to write or maintain tests for unused code. In the previous course, we discussed lean software development and how work in process or WIP is a leading indicator of delivery time. By keeping WIP down, delivery time is kept down. If developers are asked to create detailed and robust test processes first, this results in fewer features being worked on concurrently. Last, code that is well tested is easier to maintain. In a complex project, introducing a bug in one part of the program could affect other parts. So having targeted tests to validate individual pieces of code is critical. Let's dig deeper into the different kinds of tests next.

Demo: Using pylint to Keep Code Clean
We've discussed CICD at a high level. So now let's talk about specific ways to achieve quality at the source. Linting is an old term used to describe the process of identifying syntax or styling issues with a piece of code. Linters don't actually execute code. They just scan it. Imagine holding a physical printout of your code at arm's length and inspecting it quickly. You'll likely see obvious styling issues, such as indentation or unintuitive variable names. You may also see syntax errors, such as a missing colon or closing curly brace. You can't quite tell if the code's logic is correct, but you can catch simple problems very quickly. Linters are a great first step in testing your code. Because they don't execute your code, there is little security risk. They may even surface some security problems ahead of time. Linters are also extremely fast, often running in seconds and providing a detailed summary of where problems exist. The recurring theme here is to fail fast and early. Linters exist for almost every programming language or syntax format. I'll be demonstrating pylint in this course. I suggest trying a few different ones, then choosing your favorite. At this point, let's use pylint to scan the Globomantics CRM app source code for any issues. In my Ansible and Python network automation courses, I discuss pylint in greater depth. But here I'll keep it higher level. Let's install it using pip first. Okay, we've installed pylint successfully. Let's try it out on our start.py file. Pylint runs pretty quickly and reveals a list of potential issues with your code. Before this clip, I introduced some bugs. Each message has a corresponding code that starts with a letter and ends with four numbers. The letter indicates the message type, such as w for warning, e for error, and a few others. The number represents the error code. In this case, we have two issues. First, I imported os on line 9 for no reason, which is a warning. Then, I attempted to use an undefined variable named bad_var on line 13, clearly an error. You'll see we have an overall code score of 6.25 out of 10. The echo $? prints the return code of the previous operation, and 0 means success. A non-0 return code makes pylint very handy in test pipelines. If linting fails, our pipeline fails, giving us fast feedback. Let's see wat git diff says. I explained how to interpret this output in the previous course. Okay, we can see the unnecessary import os and the undefined variable. The easiest solution to fix this is to simply discard these changes using git checkout -- on the file. Let's run pylint once more. Okay, now we see no error messages and a perfect 10 out of 10 score. The return code is 0 now, so the Bash process knows linting succeeded. I want to finish up by introducing you to the config file for pylint. Sometimes you'll want to enable stricter or more relaxed rules depending on your goals. We can generate an rc file using the callout. But instead, I'll open the file I already created, and the conventional name is .pylintrc. I won't dig into the details here. But as a programmer, you can customize how pylint operates. The file is well- commented, and many of the options become self-evident after reading about them. Next, let's talk about unit testing, the next stage in our pipeline.

Demo: Real-life TDD by Writing Unit Tests with pytest
Next, let's talk about unit testing. In a sentence, pytest is a simple framework for unit testing your Python code. It's comparable to JUnit for Java applications. It is both a Python module that can be imported and also a binary that can be invoked from the shell. Keep it simple. For example, each method you define in a class should have a corresponding unit test with a number of test cases being checked. The primary goal is to build good test coverage and catch simple problems. Pytest isn't the only solution for Python unit testing. There is another package called unittest, which feels a lot more like Java than Python to be honest. We won't go through it in the demo, but I'll include unittest examples in the course files, so you can compare and contrast the two approaches. I recommend you explore these solutions and select the one most suitable for your project. Let's explore unit testing for our CRM app by combining the TDD process with the pytest tool. Suppose we want to add a method to our database class that returns true if clients owe Globomantics money and false otherwise. All we have to do is check if the balance is greater than 0. Right now, we have a method stub, but no actual implementation. Because the owes_money method does nothing at all, we expect any test run against it to fail. Let's examine the test for this method, assuming it already works, even though clearly it doesn't. I've written them in the test_unit.py file. I'm importing the pytest and database modules here since we need access to our model data, as well as test features. This first function is a pytest fixture, meaning that it automatically runs before each test. In short, we want to initialize our database by reading from the db.json file so we don't have to manually do it in every test function. We can access this data using db_mock, which is automatically passed into our test cases. The first test case targets the database balance method, which will return a usd- formatted string in response to an account ID input. Given our JSON database, if we pass in account 100, we should see 40 USD. I'm just enumerating the expected outputs for all the other accounts. If any of these assert statements evaluate to false, either our balance method has a bug or our data is incorrect. It's important to perform negative testing too like passing in a bogus account ID and ensuring none is returned. This method of checking outputs against static inputs is sometimes called stubbing. The second test case targets the new owes_money method. Given that account 100 carries a balance of $ 40.00, and the other accounts had 0 or negative balances, only account 100 owes Globomantics money. Don't forget negative testing using bogus accounts. Let's run the test using pytest --verbose and see what happens. Pytest discovered two test items. The test for the balance method passed, but the owes_money test failed. This is the expected result and the whole point of test-driven development. Given that the owes_money method didn't return anything, asserting none evaluates to false. Behind the scenes, I've quickly implemented this method to save time. Let's explore the updates. The code is very simple. First, I get the account data by passing in the account ID. The account data contains a due key and a paid key. Computing the difference yields the balance. If it's a positive number, the method returns true. If it's 0 or less, the methods returns false. If the account ID isn't found at all, it returns none. Let's run our test again and ensure it works. This looks great. Now we've used test- driven development to write code with the added benefit of having good unit tests already in place. Let's test the entire app as a whole in the next few clips.

Demo: System Testing Using Docker and pytest
As the name implies, system tests validate that the entire application functions correctly. Pytest isn't just for unit tests. For a system test, we could spin up our CRM Docker containers, then use pytest to execute a workflow that tests the functionality of our CRM app as a whole. Specifically, we can integrate the popular Python request library that we've been using for much of this course. Each individual test can make HTTP get or post requests to the different Docker containers running locally, then check the HTTP status code and body for the expected result. We can include negative tests too, such as a get request to a nonexistent URL, then assert the status code returned equals 404. System tests are both necessary and powerful, but are slow and more complex to set up. Linters and unit tests run very quickly and don't take long to design and implement. System tests take more planning and a setup time. This is why we run them after the faster tests have already passed. Let's combine Docker, pytest, and the requests library to build a solid system test for the Globomantics CRM app. Let's waste no time and dive right in to our system test, which I've named test_system.py. We are using the requests library again, except rather than interact with Cisco product REST APIs, we'll use it to talk to our dockerized CRM app. The first test case simulates a user navigating to the CRM web page using HTTP get. If you recall from the previous module, our docker-compose infrastructure spun up three CRM containers using TCP ports 5001, 5002, and 5003. After defining some basic HTTP headers for HTML text, I use a for loop to iterate over those three TCP ports. Within the loop, I issue an HTTP get request to the local host using the different port numbers, allowing me to test all three containers. Once the response comes back, I assert two things. First, we must get status code 200, not just any non-error code. I am also asserting that the text Enter account ID appears inside the body, which indicates our web form has the correct prompt. Remember that negative testing is also important. In the next test, everything is the same except the last few lines. I try to navigate to an incorrect URL using bad.html as the target. I expect to get an HTTP 404 status code in response, and the words Not Found should be present in the body. This ensures that our website is behaving correctly. You might think that I'm doing too much copy/paste between test cases, and you'd be right. We can use functional decomposition to write tests. To handle HTTP post requests, I've defined an internal function named _post_acct to handle the heavy lifting. It takes a dictionary that expects an account ID and an optional account balance if the account is valid. Notice that the actual test cases are only one line long, and they just pass in different dictionaries to the helper. In the good case, account 100 should have a balance of $ 40.00. However, nick123 should be invalid, and no expected balance is supplied. In the helper function, I define some basic headers, which are needed for the HTTP post. You can always reference the Wireshark captures from the previous course if you'd like to see how I reverse-engineered these data constructs. I then use the for loop to step over each TCP port issuing the post request in sequence. The body of the post request isn't JSON, but is just plain text containing a key value pair that the controller will process. The post should return HTTP status 200 every time. If an account balance was specified, we assert that the specific balance is present in the HTTP response body. If the account was invalid and we didn't expect any balance, let's ensure we see the friendly Unknown account number text in the response instead. To run this, I'm going to use docker-compose up with the --detach option, allowing us to keep typing commands. We can use docker container ls to get a list of running containers. Despite the ugly output, we can see all three containers are up. They are also running on their assigned TCP ports as expected. Let's run our system test now using pytest in verbose mode. Excellent. All four test cases have passed. I'm reasonably confident that our app is working correctly. Let's do a clean teardown using docker-compose down. In the next clip, we'll automate this entire test pipeline using Travis CI.

Demo: Reviewing a Basic Test Pipeline in Travis CI
The Globomantics sales team is thrilled at the prospect of IT being able to deliver the CRM software faster. We've already written all the tests, so why not arrange them into an automated process? Let's briefly review a continuous integration pipeline for the CRM app using Travis CI. In order to use any CICD system, you'll generally need to create a specification file. For Travis CI, that file is .travis .yml. These are typically written in YAML, so let's check it out. At the top of the file, we specify four important environmental settings. First, Travis suggests using the Xenial Linux distribution for Python 3.7 or newer. Naturally, we need to specify the desired Python version. Using the services key, we specify that we need Docker support. This gives access to both docker and docker-compose commands inside the virtual test environment. The rest of this file is like a Bash script broken into stages. At the install stage, Travis installs all required Python packages from a file. This file just contains four Python packages, one per line, as shown in orange. The before_script stage contains tasks where if any of them fail, the entire pipeline stops. Here we want to lint all Python files first, followed by unit tests. Then we want to run system test preparation by spinning up Docker containers and listing them for logging purposes. Next, we use the script stage to run our system tests. If we had multiple commands here, they would all run regardless of how the previous commands exited. Finally, I personally prefer an orderly teardown after the script, although this isn't necessary since Travis will clean up the entire test environment. I hope you noticed that these commands mirror the process we ran manually on our DevBox. So it's likely that this process will work again. I made a small and insignificant change to the readme. I already committed these changes to my local master branch, so I'm showing the difference between my local master and my remote master. This gives us the opportunity to push up a new build. I don't want to detail the button-clicking of Travis, but here's a screenshot of how to enable CI on this repository. We just click the slider and include the .travis .yml file, then push up our code. Looks like we are ready to git push to GitHub, so let's quickly do that. Next, we jump over to Travis CI and watch the testing in real time. Here is my personal Travis CI home page. You can see all my unrelated public repos over here, and the Globomantics CI repo is colored yellow with a spinning icon to indicate testing is in process. I'll fast-forward through this, and then we'll review the log. Testing completed successfully in 44 seconds, and we can see all the commit details too. The fact that this is green with a checkmark is a success indicator. Let's scroll down. First, we see all the system build information, including our selected distribution and language. Let's move on. Here, Travis just follows our specification file. First, it starts Docker as we'll need it later. It also clones our Globomantics CI repo from GitHub as it needs to test the source code we just pushed. This uses a webhook behind the scenes. It then installs our packages using pip as part of the install phase. You can see the phase names on the right. I won't read every line here, but I will say that you can expand the details from each item by expanding the left arrow. For example, expanding the details of the unit test execution shows us the regular shell output. The testing continues until everything is done, and notice that the script stage is automatically expanded as this is considered to be the most important stage. At the very end, we see that testing is done with code 0, meaning everything worked. The point of this clip isn't to make you a Travis or a GitHub expert. I wanted to show you a real-live demonstration of a test pipeline by integrating the tools and techniques you've already learned. Let's wrap up the module in the next clip.

Module Review
This was an action-packed module that introduced one of IT's hottest concepts, known as DevOps. We first discussed what DevOps is and what it is not. At a foundational level, DevOps addresses the business purpose, the company culture, the delivery processes, and the technology used to achieve the desired outcomes. Then, we explored different kinds of tests. This included syntax checkers or linters, unit tests, system tests, and user acceptance tests. CICD pipelines are complex at first, and you'll want to practice setting them up on your existing code projects. Years ago, I spent a weekend doing exactly that for many of my public repositories, and it helped me gain confidence. In the final module, we'll dig into security considerations for application development. See you there.

Understanding Web Application Threats and Mitigations
Introducing OWASP
To round out this course, we'll explore a variety of security topics so you understand and appreciate what it takes to build secure applications. Here's our agenda today. I'll begin by introducing OWASP and their mission. If you've never heard of them, I'll get you up to speed quickly. To keep things fun, we'll conduct an injection attack, a very common vulnerability and something easily demonstrated in the Globomantics CRM app. Although OWASP identifies secret data exposure as a threat, I want to dig a little deeper into this critical subject. Last, we can beef up our application deployments by integrating some external components for added security. I want to focus on web application security, but first let's discuss an organization known as OWASP. The open web application security project is a nonprofit company designed to help web developers build more secure apps. They provide free articles, tools, documentation, and other assets to the community. This has earned them immense public respect and credibility. They regularly maintain a top-10 list of most common attacks, and the most recent list is summarized in the next clip. This list is updated every few years. I'll illustrate each attack at a high level as it relates to the Globomantics CRM app. Here's a reference slide using a graphic from the OWASP website that shows the changes between the 2013 and 2017 reports. We are focused on the 2017 report, which is the newest version at the time of this recording. Feel free to pause the video if you'd like to take a closer look. I've also included a link at the bottom if you'd like to see the full write-up on these vulnerabilities, which is definitely worth your time.

Exploring the OWASP Top Ten Vulnerabilities
Let's run through the OWASP top 10 vulnerability list. The most common threat is an injection attack. This involves an attacker sending bogus data to the web app perhaps through an HTTP post or put operation. The attacker attempts to inject data into unauthorized places by embedding code into web forms. This can cause database overwriting or corruption. The MVC design pattern helps mitigate this attack given the separation of duties between components. Input data validation is also key in sharing that when prompted for data, the user doesn't try to inject code. Broken authentication is everywhere. Imagine storing passwords in plaintext or hashing them with legacy algorithms. This makes unauthorized entry much easier for attackers. Even worse, completely broken authentication, such recycling sessions IDs or improper token validation can also allow attackers in. Deploying multifactor authentication or MFA is a good first step followed by password complexity rules and timed rotation periods. Additionally, administrators can limit failing attempts with time-based lockouts to prevent brute force attacks. No one likes having their sensitive data compromised, yet it's in the news constantly. There are two classifications of data, in transit and at rest. Data in transit can be secured using HTTP Secure or HTTPS typically using digital certificates for security. Most public websites today use this to encrypt traffic across the internet. Data at rest can be encrypted to prevent data exfiltration. There are a number of commercial products and cryptographic coding techniques to address this. Also, sensitive data should never be stored unnecessarily. If you don't need it, delete it and remove the liability of protection. Similar to injection attacks, an XML external entity or XXE may embed malicious code within XML. This is processed by the web server, and rather than referencing a valid entity, the XML document references something bogus. For example, the poorly built XML parser could blindly dereference bogus variables or give access to arbitrary files. By running the XML code, the server could fulfill illegitimate requests for information, such as the Linux password file containing sensitive user data. Two relatively easy mitigations include using a less complex encoding, such as JSON, or disabling the external entity feature within XML on the server. Broken access control is present when attackers can access unauthorized resources. Suppose our user requests his account balance. Presumably, only he should have access. If an attacker issues the same HTTP request, it should fail, but only if the app is programmed correctly. In security, the principle of least privilege requires granting the minimum required access based on the user's role. Deny by default is the standard. Additional controls include reducing external accessibility, removing unnecessary accounts, hiding metadata, and disabling unnecessary services. Security misconfigurations are common, but completely preventable. These often result by failing to remove default configurations. This could include default credentials, insecure and unnecessary services, and insecure communications protocols. At a coding level, it might mean unnecessary or untested code being used in production or perhaps unprotected secrets. The mitigation for this attack is to have a repeatable hardening process and a segmented application architecture perhaps using the MVC design pattern. You'll need good security compliance controls to stay on top of this one. Cross-site scripting or XSS is a very common type of injection attack. Like all injection attacks, the attacker plants malicious code, usually JavaScript, on a web server. However, cross-site scripting targets unsuspecting users as the client's browser will execute the code if they access a given resource. This bypasses the need for phishing as the code is embedded in a legitimate website. This would grant the attacker access to everything the browser knows, such as sessions tokens, allowing attackers to bypass authorization controls. It's often used in man-in-the-middle attacks. This example creates an annoying pop-up, but a real attacker would use the session data maliciously. Cross-site request forgery or CSRF isn't on the OWASP top 10 list anymore, but it is sometimes confused with cross- site scripting. In CSRF, suppose you've just performed a bank transaction, then decided to read your favorite blog in another browser tab. If you decided to comment on a blog post and click Submit, what happens? You probably assumed that it post your blog comment, but it could also access your bank site because that session is still authenticated. To mitigate this, web developers should add hashes to their web forms. Additionally, consider using the HTTP refer header, which can ensure new HTTP requests are coming from the original site, not cross-site. On the client side, be sure to close unused tabs and log out of sessions. Serialization is converting objects to byte strings to store or transport data while deserialization converts it back so it can be meaningfully consumed. Our Help Desk employee is pleased to see that he's paid off his personal loans and carries no balance with his lender. Imagine if an attacker could intercept bytes in transit, deserialize it back into its useable form, edit the data, serialize it again, and send it onwards. This man-in-the-middle attack could be used for unauthorized privilege escalation or remote code execution on the target. It could also manipulate user data as shown here. To mitigate against this, log all serialization and deserialization failures since it may take attackers a while to figure out how to do this correctly. Also, apply integrity checking by hashing data to minimize the risk of unauthorized rights. We're all guilty of this one, deferring an update by saying remind me later because we were focused on something else. This technical debt can accrue and cause issues later, and it's true for software, systems, and network people. Always keep your systems patched and on the most secure versions available. You'll also want to remove legacy or unsupported products for which security updates will never come. Attackers can easily discover and exploit these issues. As a relevant example, Python 2 is being retired in 2020, so you better migrate to Python 3 before then. Logging isn't an exciting topic, but it's a critical one. It's hard to secure what you can't see and even harder to react to an attack while blind. Because 100% security isn't attainable, this added visibility really matters. You'll want to keep an audit trail of activity, which feeds into a security information and event management or SIM system. These products digest raw data and provide actual information. That wraps up our summary of the top 10 OWASP threats. I suggest you explore some modern, real-life use cases for each one to further your understanding.

Demo: CRM Injection Attack in Action
A junior developer added some new functionality to our CRM app and asked us, as the team lead, to perform a code review. Let's do it, but with a focus on security. Let's take a high-level look at our project to review by checking two levels deep into the directory structure. The Globomantics CRM app is based on the MVC design pattern, and the start.py file is the controller component. This is where the junior engineer has made changes. He hasn't committed the changes and has asked us to give it a quick review. Let's check out git diff on start.py to see his changes. This engineer modified the original logic to collect user input and process the account ID lookups. The code is now wrapped in a try accept block. This allows programmers to run code that may raise errors, but gives us the ability to handle them without crashing the program. After collecting the user input, we are now using the exec function and passing in whatever the user supplied. This function attempts to run the input string as a line of Python code, which is extremely dangerous. If the user enters a string like the one that is highlighted, it would overwrite data entries and lead to incorrect accounting data. If the user enters a valid account number, such as account 100, that isn't a Python line of code and will generate an error. This new code just catches the error and does nothing, which is also dangerous due to insufficient logging, another key vulnerability. The finally block is executed no matter what, which contains the original account processing logic. Let's run this code. Next, I'll switch to the web browser so we can test it out. Let's suppose account 100 is our personal account. Globomantics says we owe them $40. To launch an injection attack, let's try to overwrite the amount we owe to 0, which should reduce our balance. This particular account originally owed 100, but already paid 60, yielding a balance of 40. If we change the Due field to 0, the balance will become -$ 60.00. Okay, that wasn't a valid account number, but the attack probably worked. Let's ask for the account 100 balance again. Now we see the post-attack results. Our balance has turned negative, and we've basically robbed Globomantics. I'll include this code sample for reference, but needless to say, we should not accept these changes from the junior engineer. In the next clip, we'll discuss some security considerations regarding password and secret management.

Options for Managing Secrets in Python
Let's discuss common techniques for handling secrets in Python using some simple code samples. The getpass module and its getpass member function are excellent for interactive programs. It ships with Python, so no pip installations are needed. It prompts the user for a password, but doesn't mirror the text. My script is quite insecure as I'm just printing out the password for illustrative purposes, but you can use the secret variable for authentication later in your code. Running the program, we see the password prompt followed by the program's output. Getpass returns a string and reveals the fancy DevOps password I typed. Another common method is using environment variables. We used this technique in the Webex Team's API demo earlier in the course, but it works well for any kind of secret. meaning operating system, which also comes standard with Python. Using the os.environ dictionary, we can access the secret environment variable. Then, we can insecurely print it out just for testing. In order for this to work, we must define an environment variable named secret, which is specific to a given shell process. It won't be checked into version control or be accessible by other users on the system. Then, we run the program and observe the output, which is the same as the previous slide. Another approach is to use the Python keyring package, which must be installed via pip. This interacts with the system's built-in key management system, and on macOS, this is the keychain application. You can create passwords in one of two ways. You can write a one-time script, as I'm showing here, to create a new keyring entry named devopstest for user krtest with our funky DevOps password. You could also build it manually using your OS. And in this case, I'm showing a screenshot of my keychain where this password exists. My laptop is a MacBook running Mohave as an example. Here's how we can consume this keyring entry. We can use keyring.get_password and specify the keyring entry name and username. Then, we can print out the secret along with the type. When we execute the program, we see the same output as the previous examples. I'll include these short samples in the course files for reference. There are countless ways to solve this problem, but I'll briefly touch on a few more. You could also store your secrets in a plaintext file, perhaps JSON or YAML formatted, then restrict it to read-only for the owner. No other users will be able to read it. Just be careful not to accidentally add it into a version control system. You could also encrypt that text file commonly called a vault. This file can be added to a version control system, which is great when there are many secrets to be stored. Individual workstations will use plaintext pass phrases to open the vault. You don't have to build vault logic from scratch. HashiCorp Vault, Ansible Vault, and many other products can assist with the protection of secrets.

Adding Supplementary Security Components and Services
Let's talk about a pair of external network devices that can help secure our web applications. A firewall is a network device that serves as a filter between trusted and untrusted networks. Cisco's Firepower Threat Defense or FTD product, which we discussed in a previous module, is one such example. A firewall can be placed in front of our web view and explicitly allow web traffic, but little else. This would prevent basic denial of service attacks like the ping of death. Intrusion prevention and detection capabilities could also inspect HTTP requests at a deeper level. Firewalls typically do not allow outside to inside unsolicited messages, which improves security further. Network firewalls are necessary, but not sufficient. We can use a reverse proxy, sometimes called a web application firewall or WAF, to protect the web servers in our data center. Clients will send their HTTP requests towards this reverse proxy, which performs web-specific security filtering, then passes valid requests on to the web servers. Using security appliances in this way makes it more difficult for attackers to exploit the OWASP vulnerabilities discussed earlier. These devices can inspect both requests and responses for bidirectional protection. Sometimes people forget that availability is an important security consideration. Let's examine our existing product chain consisting of a network firewall and a reverse proxy. DNS, or domain name system, at a basic level, is a mechanism to map names to network addresses. We'll cover DNS in greater depth in a future course. One advantage of DNS is being able to use a common name, such as www.globomantics .com to represent different IP addresses. Suppose Globomantics wants to scale their CRM app. They build multiple data centers, then use DNS to loadshare between the two. Or perhaps they use DNS to select one as active and one as standby. Both solutions are possible. But even within a data center, we can use a load balancer in conjunction with DNS to achieve scalability also. The load balancer can monitor the utilization across multiple servers, then shuttle incoming requests to the proper device. Now Globomantics can achieve availability intrasite and intersite using a combination of DNS and load balancers.

Course Summary
I want to share some final thoughts with you as we wrap up the course. You might be feeling overwhelmed by the sheer number of new products and APIs this course has introduced. As you've seen in the demos, while the product use cases may differ, the REST APIs supported by most products are quite similar. Once you master the basics of REST, HTTP, and Python programming, you can easily interact with any of them after a few short minutes of reading the documentation. When it comes to deploying and securing applications, there are many different options to consider. Should we use containers or virtual machines? Public cloud or private or hybrid? Which OWASP threats are the most important to address first in our business? What kind of CICD strategy should be implement? And how much test coverage do we need? These are difficult questions that often take weeks or months to fully answer. My advice is to start that conversation early and adjust as you build the app. If you want to learn these technologies in greater detail, remember that Pluralsight has deep-dive courses on almost everything I've discussed in this course. You can always reach out to me for help on Twitter or in the course discussion. I look forward to hearing from you, and I sincerely thank you for viewing this course.
